[
  {
    "objectID": "personal_blog.html",
    "href": "personal_blog.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "",
    "text": "A distributed processing system which is mainly used for large sizes of data sets\nProvides multiple libraries:\n\nParallel processing\nMachine learning\netc.\n\nSpark Structure on Cluster of Computers\n\nDriver communicates with cluster manager to get worker nodes\nCluster manager allocates the necessary resources (nodes) and assigns pieces of the total process\nWorker nodes perform given tasks (pieces of a larger process) and return the result to the Driver node"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#counting-operations",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#counting-operations",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Counting operations",
    "text": "Counting operations\n\nfrom pyspark.sql.functions import countDistinct\n\nnum_teams = df.select(countDistinct('Team')).collect()[0][0]\nnum_teams # Simply returns number of distinct teams\n\n30\n\n\n\ndf.groupBy(\"Team\").count().show(5)\n# groups df by team, counts each occurance of team (so num players) and displays first 5\n\n+--------------------+-----+\n|                Team|count|\n+--------------------+-----+\n|        Phoenix Suns|   15|\n|      Boston Celtics|   16|\n|    Dallas Mavericks|   13|\n|New Orleans Pelicans|   16|\n|       Brooklyn Nets|   17|\n+--------------------+-----+\nonly showing top 5 rows"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#ordering-operations",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#ordering-operations",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Ordering operations",
    "text": "Ordering operations\n\ndf.orderBy('Name').show(5) # Sorts asc (by default) by single col\n\n+-----------------+--------------------+--------+--------+--------+\n|             Name|                Team|Position|Birthday|  Salary|\n+-----------------+--------------------+--------+--------+--------+\n|     Aaron Gordon|       Orlando Magic|      PF| 9/16/95|19863636|\n|    Aaron Holiday|      Indiana Pacers|      PG| 9/30/96| 2239200|\n|      Abdel Nader|Oklahoma City Thu...|      SF| 9/25/93| 1618520|\n|      Adam Mokoka|       Chicago Bulls|       G| 7/18/98|   79568|\n|Admiral Schofield|  Washington Wizards|      SF| 3/30/97| 1000000|\n+-----------------+--------------------+--------+--------+--------+\nonly showing top 5 rows\n\n\n\n\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc('Salary')).show(5) # sort by salary desc\n\n+-----------------+--------------------+--------+--------+--------+\n|             Name|                Team|Position|Birthday|  Salary|\n+-----------------+--------------------+--------+--------+--------+\n|    Stephen Curry|Golden State Warr...|      PG| 3/14/88|40231758|\n|Russell Westbrook|     Houston Rockets|      PG|11/12/88|38506482|\n|       Chris Paul|Oklahoma City Thu...|      PG|  5/6/85|38506482|\n|        John Wall|  Washington Wizards|      PG|  9/6/90|38199000|\n|     James Harden|     Houston Rockets|      PG| 8/26/89|38199000|\n+-----------------+--------------------+--------+--------+--------+\nonly showing top 5 rows\n\n\n\n\ndf.orderBy(['Team', desc('Salary')]).show(5)\n# sort by multiple cols\n\n+----------------+-------------+--------+--------+--------+\n|            Name|         Team|Position|Birthday|  Salary|\n+----------------+-------------+--------+--------+--------+\n|Chandler Parsons|Atlanta Hawks|      SF|10/25/88|25102512|\n|     Evan Turner|Atlanta Hawks|      PG|10/27/88|18606556|\n|    Allen Crabbe|Atlanta Hawks|      SG|  4/9/92|18500000|\n| De'Andre Hunter|Atlanta Hawks|      SF| 12/2/97| 7068360|\n|   Jabari Parker|Atlanta Hawks|      PF| 3/15/95| 6500000|\n+----------------+-------------+--------+--------+--------+\nonly showing top 5 rows"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#adding-columns",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#adding-columns",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Adding columns",
    "text": "Adding columns\n\n# Adding columns with withcolumn()\nfrom pyspark.sql.functions import col\ndf_add = df.withColumn('salary_k', col('Salary')/ 1000).show(5)\n\n+--------------+------------------+--------+--------+-------+--------+\n|          Name|              Team|Position|Birthday| Salary|salary_k|\n+--------------+------------------+--------+--------+-------+--------+\n|  Shake Milton|Philadelphia 76ers|      SG| 9/26/96|1445697|1445.697|\n|Christian Wood|   Detroit Pistons|      PF| 9/27/95|1645357|1645.357|\n| PJ Washington| Charlotte Hornets|      PF| 8/23/98|3831840| 3831.84|\n|  Derrick Rose|   Detroit Pistons|      PG| 10/4/88|7317074|7317.074|\n| Marial Shayok|Philadelphia 76ers|       G| 7/26/95|  79568|  79.568|\n+--------------+------------------+--------+--------+-------+--------+\nonly showing top 5 rows"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#removing-columns",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#removing-columns",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Removing columns",
    "text": "Removing columns\n\n# Removing cols with drop()\ndf_drop = df.drop('Salary').show(5)\n\n+--------------+------------------+--------+--------+\n|          Name|              Team|Position|Birthday|\n+--------------+------------------+--------+--------+\n|  Shake Milton|Philadelphia 76ers|      SG| 9/26/96|\n|Christian Wood|   Detroit Pistons|      PF| 9/27/95|\n| PJ Washington| Charlotte Hornets|      PF| 8/23/98|\n|  Derrick Rose|   Detroit Pistons|      PG| 10/4/88|\n| Marial Shayok|Philadelphia 76ers|       G| 7/26/95|\n+--------------+------------------+--------+--------+\nonly showing top 5 rows"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#renaming-columns",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#renaming-columns",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Renaming columns",
    "text": "Renaming columns\n\n# Renaming columns with withColumnsRenamed()\ndf_ren = df.withColumnRenamed('Team', 'Team Name')\ndf_ren.show(5)\n\n+--------------+------------------+--------+--------+-------+\n|          Name|         Team Name|Position|Birthday| Salary|\n+--------------+------------------+--------+--------+-------+\n|  Shake Milton|Philadelphia 76ers|      SG| 9/26/96|1445697|\n|Christian Wood|   Detroit Pistons|      PF| 9/27/95|1645357|\n| PJ Washington| Charlotte Hornets|      PF| 8/23/98|3831840|\n|  Derrick Rose|   Detroit Pistons|      PG| 10/4/88|7317074|\n| Marial Shayok|Philadelphia 76ers|       G| 7/26/95|  79568|\n+--------------+------------------+--------+--------+-------+\nonly showing top 5 rows"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#aggregations-and-summary-statistics",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#aggregations-and-summary-statistics",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Aggregations and Summary Statistics",
    "text": "Aggregations and Summary Statistics\n\n# Aggregations and Summary Stats\n\ndf.selectExpr(\n    'mean(Salary) as mean_sal',\n    'min(Salary) as min_sal',\n    'max(Salary) as max_sal',\n    'stddev_pop(Salary) as std_salary'\n).show()\n\n+-----------------+-------+--------+-----------------+\n|         mean_sal|min_sal| max_sal|       std_salary|\n+-----------------+-------+--------+-----------------+\n|7653583.764444444|  79568|40231758|9278483.657952718|\n+-----------------+-------+--------+-----------------+"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#converting-data-types",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#converting-data-types",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Converting Data Types",
    "text": "Converting Data Types\n\n# Using cast() to change dtype of Salary column to integer\ndf_int = df.withColumn(\"Salary_int\", col(\"Salary\").cast(\"int\"))\ndf_int.dtypes\n\n[('Name', 'string'),\n ('Team', 'string'),\n ('Position', 'string'),\n ('Birthday', 'string'),\n ('Salary', 'bigint'),\n ('Salary_int', 'int')]\n\n\n\n# Using to_date() to cast \"Birthday\" col to date dtype\nfrom pyspark.sql.functions import to_date\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") # Done if using years before 2000, as this df is\ndf_date = df.withColumn(\"DOB\", to_date(\"Birthday\", \"M/d/yy\"))\ndf_date.dtypes\n\n[('Name', 'string'),\n ('Team', 'string'),\n ('Position', 'string'),\n ('Birthday', 'string'),\n ('Salary', 'bigint'),\n ('DOB', 'date')]"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#filtering",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#filtering",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Filtering",
    "text": "Filtering\n\n# Filtering by a condition\n\ndf_cond = (\n    df\n    .filter(col(\"Salary\") &gt; 30000000)\n    .orderBy(desc(\"Salary\"))\n    .show(5)\n)\n\n+-----------------+--------------------+--------+--------+--------+\n|             Name|                Team|Position|Birthday|  Salary|\n+-----------------+--------------------+--------+--------+--------+\n|    Stephen Curry|Golden State Warr...|      PG| 3/14/88|40231758|\n|       Chris Paul|Oklahoma City Thu...|      PG|  5/6/85|38506482|\n|Russell Westbrook|     Houston Rockets|      PG|11/12/88|38506482|\n|        John Wall|  Washington Wizards|      PG|  9/6/90|38199000|\n|     James Harden|     Houston Rockets|      PG| 8/26/89|38199000|\n+-----------------+--------------------+--------+--------+--------+\nonly showing top 5 rows\n\n\n\n\n# Filtering using isin()\n\ndf_isin = (\n    df\n    .filter(col('Team').isin('Houston Rockets', 'Washington Wizards', 'Miami Heat'))\n    .show(5)\n)\n\n+---------------+------------------+--------+--------+--------+\n|           Name|              Team|Position|Birthday|  Salary|\n+---------------+------------------+--------+--------+--------+\n|  Kendrick Nunn|        Miami Heat|      SG|  8/3/95| 1416852|\n|  Rui Hachimura|Washington Wizards|      PF|  2/8/98| 4469160|\n|Michael Frazier|   Houston Rockets|       G|  3/8/94|   79568|\n|   Bradley Beal|Washington Wizards|      SG| 6/28/93|27093018|\n|  Thomas Bryant|Washington Wizards|       C| 7/31/97| 8000000|\n+---------------+------------------+--------+--------+--------+\nonly showing top 5 rows\n\n\n\n\n# Filtering using between()\ndf_btwn = (\n    df\n    .filter(col(\"Salary\").between(2000000, 2050000))\n    .show()\n)\n\n+--------------------+--------------------+--------+--------+-------+\n|                Name|                Team|Position|Birthday| Salary|\n+--------------------+--------------------+--------+--------+-------+\n|        Torrey Craig|      Denver Nuggets|      SF|12/19/90|2000000|\n|          Trey Burke|  Philadelphia 76ers|      PG|11/12/92|2028594|\n|         Noah Vonleh|Minnesota Timberw...|       C| 8/24/95|2000000|\n|        Ben McLemore|     Houston Rockets|      SG| 2/11/93|2028594|\n|        Troy Daniels|  Los Angeles Lakers|      SG| 7/15/91|2028594|\n|        Mike Muscala|Oklahoma City Thu...|       C|  7/1/91|2028594|\n|      Caleb Swanigan|    Sacramento Kings|      PF| 4/18/97|2033160|\n|       Dylan Windler| Cleveland Cavaliers|      GF| 9/22/96|2035800|\n|       Edmond Sumner|      Indiana Pacers|      PG|12/31/95|2000000|\n|       Iman Shumpert|       Brooklyn Nets|      PG| 6/26/90|2031676|\n|Michael Carter-Wi...|       Orlando Magic|      PG|10/10/91|2028594|\n+--------------------+--------------------+--------+--------+-------+"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#dealing-with-missing-values",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#dealing-with-missing-values",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Dealing with Missing Values",
    "text": "Dealing with Missing Values\n\n# Check for Missing Vals using isNull() and isNotNull()\n\ndf_n1 =(\n    df\n    .filter(col(\"Salary\").isNull())\n    .count()\n)\ndf_n1\n\n0\n\n\n\ndf_n1 =(\n    df\n    .filter(col(\"Salary\").isNotNull())\n    .count()\n)\ndf_n1\n\n450\n\n\n\n# Dropping rows with Null values using .na.drop()\n\n  # Dropping ANY row that has a null value in ANY columns - default setting of how parameter\ndf_any = (\n    df.na.drop()\n)\n\n  # Add in how = 'all' param to drop rows that have ALL columns null\ndf_all = (\n    df.na.drop(how = 'all')\n)\n\n  # Dropping with a subset, will only drop rows that have null in Name and Team cols\ndf_sub = (\n    df.na.drop(subset = ['Name', 'Team'])\n)\n\n\n# Replacing Null values using na.fill()\n\n  # Filling a specific cols nulls with a value\ndf_fill = (\n    df.na\n    .fill(value = -999, subset = ['Salary'])\n)\n\n  # Filling more than one col with a dict\ndf_fill_mult = (\n    df.na\n    .fill({'Salary': -999,\n           'Team': 'N/A'})\n)"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#duplicate-operations",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#duplicate-operations",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Duplicate operations",
    "text": "Duplicate operations\n\n# distinct() method will return a new df with duplicate rows removed, only unique obs left\ndf_dist = (\n    df\n    .select('Team', 'Name')\n    .distinct\n)\n\n\n# Using dropDuplicates() with and w/out subset\n\ndf_drop = df.dropDuplicates() # will remove all rows that are exact duplicates across all columns\n\ndf_drop_sub = df.dropDuplicates(['Team']) # Will remove rows that have duplicates in the Team column (not a good thing to do here, but shows what it does)"
  },
  {
    "objectID": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#group-operations-window",
    "href": "danl_320_mats/pyspark_basics_post/pyspark_basics_hw_1.html#group-operations-window",
    "title": "PySpark Basics | Hw 1 Post",
    "section": "Group operations (& Window)",
    "text": "Group operations (& Window)\n\ngroupBy() method is used to work with data at a grouped level\nReturns a GroupedData object\n\n\ndf_groups = df.groupBy('Position')\ndf_groups # Below shows that df_groups is a GroupedData object\n\nGroupedData[grouping expressions: [Position], value: [Name: string, Team: string ... 3 more fields], type: GroupBy]\n\n\n\n# Showing number of groups\ndf_groups.count().show()\n\n+--------+-----+\n|Position|count|\n+--------+-----+\n|      FC|    1|\n|      PF|   96|\n|       F|    5|\n|      PG|   98|\n|      SF|   76|\n|       C|   88|\n|      SG|   74|\n|       G|   10|\n|      GF|    2|\n+--------+-----+\n\n\n\n\n# Using agg functions for each group\ndf_groups.avg(\"Salary\").show()\ndf_groups.sum(\"Salary\").show()\n\n+--------+-----------------+\n|Position|      avg(Salary)|\n+--------+-----------------+\n|      FC|          79568.0|\n|      PF|        7223613.5|\n|       F|        2322338.4|\n|      PG|        9781712.0|\n|      SF|7466574.565789473|\n|       C|9686050.784090908|\n|      SG|4781786.243243244|\n|       G|         372833.4|\n|      GF|        1467055.0|\n+--------+-----------------+\n\n+--------+-----------+\n|Position|sum(Salary)|\n+--------+-----------+\n|      FC|      79568|\n|      PF|  693466896|\n|       F|   11611692|\n|      PG|  958607776|\n|      SF|  567459667|\n|       C|  852372469|\n|      SG|  353852182|\n|       G|    3728334|\n|      GF|    2934110|\n+--------+-----------+\n\n\n\n\n# Group Agg with many cols\nfrom pyspark.sql.functions import min, max, mean\n\nteam_group = (\n    df.groupBy('Team').agg(\n        min(\"Salary\").alias('min_sal'),\n        max('Salary').alias('max_sal'),\n        mean(\"Salary\").alias(\"mean_sal\")\n    ).show(5)\n)\n\n+--------------------+-------+--------+-----------------+\n|                Team|min_sal| max_sal|         mean_sal|\n+--------------------+-------+--------+-----------------+\n|        Phoenix Suns|  79568|27285000|6791594.066666666|\n|      Boston Celtics|  79568|32742000|       7238863.25|\n|    Dallas Mavericks|  79568|27285000|7432050.846153846|\n|New Orleans Pelicans|  79568|26131111|      6512430.125|\n|       Brooklyn Nets|  79568|37199000|7215064.764705882|\n+--------------------+-------+--------+-----------------+\nonly showing top 5 rows\n\n\n\n\n# Using a window function to add group level stats to og data frame\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import avg\ndf_ = df\n\nw = Window.partitionBy(\"Team\")\n\ndf_w_mean = df_.withColumn(\n    'mean_salary_by_team',\n    avg(col(\"Salary\")).over(w)\n).show()\n\n+----------------+--------------+--------+--------+--------+-------------------+\n|            Name|          Team|Position|Birthday|  Salary|mean_salary_by_team|\n+----------------+--------------+--------+--------+--------+-------------------+\n|   Kevin Huerter| Atlanta Hawks|      SG| 8/27/98| 2636280|  6503699.866666666|\n|     Evan Turner| Atlanta Hawks|      PG|10/27/88|18606556|  6503699.866666666|\n|    John Collins| Atlanta Hawks|      PF| 9/23/97| 2686560|  6503699.866666666|\n|    Vince Carter| Atlanta Hawks|      PF| 1/26/77| 2564753|  6503699.866666666|\n|Chandler Parsons| Atlanta Hawks|      SF|10/25/88|25102512|  6503699.866666666|\n|    Damian Jones| Atlanta Hawks|       C| 6/30/95| 2305057|  6503699.866666666|\n|    Allen Crabbe| Atlanta Hawks|      SG|  4/9/92|18500000|  6503699.866666666|\n|     Cam Reddish| Atlanta Hawks|      SF|  9/1/99| 4245720|  6503699.866666666|\n|   Charlie Brown| Atlanta Hawks|      SG|  2/2/97|   79568|  6503699.866666666|\n| De'Andre Hunter| Atlanta Hawks|      SF| 12/2/97| 7068360|  6503699.866666666|\n| Brandon Goodwin| Atlanta Hawks|      PG| 10/2/95|   79568|  6503699.866666666|\n|  Tyrone Wallace| Atlanta Hawks|      PG| 6/10/94| 1620564|  6503699.866666666|\n|   Jabari Parker| Atlanta Hawks|      PF| 3/15/95| 6500000|  6503699.866666666|\n|        Alex Len| Atlanta Hawks|       C| 6/16/93| 4160000|  6503699.866666666|\n|  Bruno Fernando| Atlanta Hawks|       C| 8/15/98| 1400000|  6503699.866666666|\n|    Marcus Smart|Boston Celtics|      PG|  3/6/94|12553571|         7238863.25|\n|  Brad Wanamaker|Boston Celtics|      PG| 7/25/89| 1445697|         7238863.25|\n|  Grant Williams|Boston Celtics|      PF|11/30/98| 2379840|         7238863.25|\n|  Tremont Waters|Boston Celtics|      PG| 1/10/98|   79568|         7238863.25|\n|    Daniel Theis|Boston Celtics|       C|  4/4/92| 5000000|         7238863.25|\n+----------------+--------------+--------+--------+--------+-------------------+\nonly showing top 20 rows"
  },
  {
    "objectID": "danl_320_mats/ben_jerry_post/danl_320_hw_2_post.html",
    "href": "danl_320_mats/ben_jerry_post/danl_320_hw_2_post.html",
    "title": "Ben & Jerry’s | Hw 2 Post",
    "section": "",
    "text": "import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when, log\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\n\nLoading the Data and Preparing for PySpark\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\nice_cream = pd.read_csv('https://bcdanl.github.io/data/ben-and-jerry-cleaned.csv')\n\ndef bool_to_int(df):\n  for i in df.columns:\n    if df[i].dtype == 'bool':\n      df[i] = df[i].astype('int')\n    else:\n      pass\nbool_to_int(ice_cream)\n\nice_cream = ice_cream[~ice_cream['tvcable'].isna()]\n\nice_cream.describe()\n\n\n  \n    \n\n\n\n\n\n\npriceper1\nhousehold_id\nhousehold_income\nhousehold_size\nusecoup\ncouponper1\nmarried\nhispanic_origin\nmicrowave\ndishwasher\nsfh\ninternet\n\n\n\n\ncount\n21940.000000\n2.194000e+04\n21940.000000\n21940.000000\n21940.000000\n21940.000000\n21940.000000\n21940.000000\n21940.000000\n21940.000000\n21940.000000\n21940.000000\n\n\nmean\n3.314440\n1.661833e+07\n125226.982680\n2.456746\n0.106746\n0.125500\n0.604239\n0.047995\n0.981632\n0.773063\n0.731860\n0.843163\n\n\nstd\n0.665394\n1.168728e+07\n57120.924532\n1.335610\n0.308797\n0.517386\n0.489025\n0.213760\n0.134282\n0.418861\n0.443001\n0.363655\n\n\nmin\n0.000000\n2.000358e+06\n40000.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.000000\n8.143519e+06\n80000.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n0.000000\n1.000000\n\n\n50%\n3.340000\n8.401765e+06\n110000.000000\n2.000000\n0.000000\n0.000000\n1.000000\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n75%\n3.590000\n3.018434e+07\n170000.000000\n3.000000\n0.000000\n0.000000\n1.000000\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n9.480000\n3.044069e+07\n310000.000000\n9.000000\n1.000000\n8.980000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nice_cream = ice_cream[['household_income', 'household_size', 'couponper1', 'region', 'married', 'priceper1']]\n\nvars_to_norm = ['household_income', 'household_size', 'couponper1']\n\ndef norm(df, cols):\n  for i in cols:\n    mean = df[i].mean()\n    std = df[i].std()\n    df[i] = (df[i] - mean) / std\n\nnorm(ice_cream, vars_to_norm)\n\n\nice_cream.describe(include='all')\n\n\n  \n    \n\n\n\n\n\n\nhousehold_income\nhousehold_size\ncouponper1\nregion\nmarried\npriceper1\n\n\n\n\ncount\n2.194000e+04\n2.194000e+04\n2.194000e+04\n21940\n21940.000000\n21940.000000\n\n\nunique\nNaN\nNaN\nNaN\n4\nNaN\nNaN\n\n\ntop\nNaN\nNaN\nNaN\nSouth\nNaN\nNaN\n\n\nfreq\nNaN\nNaN\nNaN\n6706\nNaN\nNaN\n\n\nmean\n-7.643030e-17\n-1.023389e-16\n-4.793087e-17\nNaN\n0.604239\n3.314440\n\n\nstd\n1.000000e+00\n1.000000e+00\n1.000000e+00\nNaN\n0.489025\n0.665394\n\n\nmin\n-1.492045e+00\n-1.090697e+00\n-2.425653e-01\nNaN\n0.000000\n0.000000\n\n\n25%\n-7.917761e-01\n-3.419754e-01\n-2.425653e-01\nNaN\n0.000000\n3.000000\n\n\n50%\n-2.665745e-01\n-3.419754e-01\n-2.425653e-01\nNaN\n1.000000\n3.340000\n\n\n75%\n7.838287e-01\n4.067463e-01\n-2.425653e-01\nNaN\n1.000000\n3.590000\n\n\nmax\n3.234769e+00\n4.899077e+00\n1.711391e+01\nNaN\n1.000000\n9.480000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nic = spark.createDataFrame(ice_cream)\nic.show()\n\n+--------------------+-------------------+-------------------+-------+-------+---------+\n|    household_income|     household_size|         couponper1| region|married|priceper1|\n+--------------------+-------------------+-------------------+-------+-------+---------+\n| 0.08355987510778587|-0.3419754294514314|  0.723830630783873|Central|      0|     3.41|\n| 0.08355987510778587|-0.3419754294514314|-0.2425653069622484|Central|      0|      3.5|\n| 0.08355987510778587|-0.3419754294514314|-0.2425653069622484|Central|      0|      3.5|\n| -0.9668432913541072|-1.0906972059372513|-0.2425653069622484|   West|      0|      3.0|\n| 0.08355987510778587| 0.4067463470343886|-0.2425653069622484|  South|      1|     3.99|\n| 0.08355987510778587| 0.4067463470343886|-0.2425653069622484|  South|      1|     3.89|\n| 0.08355987510778587| 0.4067463470343886|-0.2425653069622484|  South|      1|     3.89|\n|-0.26657451371284513|-1.0906972059372513|-0.2425653069622484|   East|      0|     2.14|\n|  1.4840974303903098|-1.0906972059372513|-0.2425653069622484|Central|      0|      3.5|\n|  1.4840974303903098|-1.0906972059372513| 2.1734245374030547|Central|      0|      3.0|\n|  1.4840974303903098|-1.0906972059372513|  1.593586974755382|Central|      0|      4.0|\n| -0.9668432913541072| 1.1554681235202087|-0.2425653069622484|  South|      1|     2.99|\n| -0.9668432913541072| 1.1554681235202087|-0.2425653069622484|  South|      1|      2.5|\n| -0.9668432913541072| 1.1554681235202087|-0.2425653069622484|  South|      1|     2.27|\n| -0.9668432913541072|-0.3419754294514314|-0.2425653069622484|   East|      1|     2.55|\n| -0.9668432913541072|-0.3419754294514314|-0.2425653069622484|   East|      1|     2.56|\n| 0.08355987510778587|-1.0906972059372513|-0.2425653069622484|   East|      0|     3.19|\n|  1.1339630415696789|-1.0906972059372513|-0.2425653069622484|Central|      0|     3.64|\n|-0.26657451371284513|-0.3419754294514314|-0.2425653069622484|Central|      0|      6.0|\n|-0.26657451371284513|-0.3419754294514314|-0.2425653069622484|Central|      0|      6.0|\n+--------------------+-------------------+-------------------+-------+-------+---------+\nonly showing top 20 rows\n\n\n\n\n\nModel Overview\n\nThe model will use marital status, coupon amount per unit, region, household income, and household size.\ncoupon amount per unit, household income, and household size are all mean normalized\nmarital status, and region will be one-hot encoded (marital status is single binary varible as it was boolean prior)\nThe label data to be predicted is price per unit\n\n\n\nData Transformation and Splitting\n\ndtrain, dtest = ic.randomSplit([0.67, 0.33], seed = 123)\n\n\ndtrain.describe().show()\n\n+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+\n|summary|    household_income|      household_size|          couponper1| region|            married|         priceper1|\n+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+\n|  count|               14653|               14653|               14653|  14653|              14653|             14653|\n|   mean|0.001444483912653...|0.004972012187744043|3.222713433651515E-4|   NULL| 0.6045860915853409|3.3151769749735025|\n| stddev|  1.0028801390568312|  1.0040586429435043|   1.010263935750957|   NULL|0.48895609764846787|0.6687969564086118|\n|    min| -1.4920448745850536| -1.0906972059372513| -0.2425653069622484|Central|                  0|               0.0|\n|    max|   3.234769374493465|   4.899077005949308|  17.113905734958088|   West|                  1|              9.48|\n+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+\n\n\n\n\ndtest.describe().show()\n\n+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+\n|summary|    household_income|      household_size|          couponper1| region|            married|         priceper1|\n+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+\n|  count|                7287|                7287|                7287|   7287|               7287|              7287|\n|   mean|-0.00290462779913...|-0.00999792707381...|-6.48036502582157E-4|   NULL| 0.6035405516673528|  3.31295839949812|\n| stddev|  0.9942455078562172|  0.9917815965837328|  0.9791034822325205|   NULL|0.48919545698711797|0.6585402074234148|\n|    min| -1.4920448745850536| -1.0906972059372513| -0.2425653069622484|Central|                  0|               0.6|\n|    max|   3.234769374493465|   4.899077005949308|   15.21976969697569|   West|                  1|               8.0|\n+-------+--------------------+--------------------+--------------------+-------+-------------------+------------------+\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\nimport numpy as np\nimport scipy.stats as stats\nfrom tabulate import tabulate\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler,\n    and inserts a dashed horizontal line after the Intercept row. The table includes separate columns\n    for the 95% confidence interval lower and upper bounds for each coefficient (computed at the 5% significance level)\n    and an \"Observations\" row (using model.summary.numInstances) above the R² row.\n    The RMSE row is placed as the last row.\n\n    The columns are ordered as:\n        Metric | Value | Significance | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    For the \"Value\", \"Std. Error\", \"95% CI Lower\", and \"95% CI Upper\" columns, commas are inserted every three digits,\n    with 3 decimal places (except for Observations which is formatted as an integer with commas).\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Extract coefficients and standard errors as NumPy arrays\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element)\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Compute t-statistics for feature coefficients (t = beta / SE(beta))\n    # t_stats = coeffs / std_errors\n    t_stats = model.summary.tValues\n\n    # Degrees of freedom: number of instances minus number of predictors minus 1 (for intercept)\n    df = model.summary.numInstances - len(coeffs) - 1\n\n    # Compute the t-critical value for a 95% confidence interval (two-tailed, 5% significance)\n    t_critical = stats.t.ppf(0.975, df)\n\n    # Compute two-tailed p-values for each feature coefficient\n    # p_values = [2 * (1 - stats.t.cdf(np.abs(t), df)) for t in t_stats]\n    p_values = model.summary.pValues\n\n    # Function to assign significance stars based on p-value\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build the table rows.\n    # Order: Metric, Value, Significance, Std. Error, p-value, 95% CI Lower, 95% CI Upper.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n        table.append([\n            \"Beta: \" + feature,       # Metric name\n            beta,                     # Beta estimate (Value)\n            significance_stars(p),    # Significance stars\n            se,                       # Standard error\n            p,                        # p-value\n            ci_lower,                 # 95% CI lower bound\n            ci_upper                  # 95% CI upper bound\n        ])\n\n    # Compute and add the intercept row with its SE, p-value, significance, and CI (if available)\n    if intercept_se is not None:\n        intercept_t = model.intercept / intercept_se\n        intercept_p = 2 * (1 - stats.t.cdf(np.abs(intercept_t), df))\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_se = \"\"\n        intercept_p = \"\"\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n\n    table.append([\n        \"Intercept\",\n        model.intercept,\n        intercept_sig,\n        intercept_se,\n        intercept_p,\n        ci_intercept_lower,\n        ci_intercept_upper\n    ])\n\n    # Append overall model metrics:\n    # Insert an Observations row using model.summary.numInstances,\n    # then an R² row, and finally the RMSE row as the last row.\n    table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table.\n    # For the \"Value\" (index 1), \"Std. Error\" (index 3), \"95% CI Lower\" (index 5), and \"95% CI Upper\" (index 6) columns,\n    # format with commas and 3 decimal places, except for Observations which should be an integer with commas.\n    # For the p-value (index 4), format to 3 decimal places.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                # Format Observations as integer with commas, no decimals.\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if i in [1, 3, 5, 6]:\n                    formatted_row.append(f\"{item:,.3f}\")\n                elif i == 4:\n                    formatted_row.append(f\"{item:.3f}\")\n                else:\n                    formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Generate the table string using tabulate.\n    table_str = tabulate(\n        formatted_table,\n        headers=[\"Metric\", \"Value\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"],\n        tablefmt=\"pretty\",\n        colalign=(\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n    )\n\n    # Insert a dashed line after the Intercept row for clarity.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n\n\nregion_dummys, region_reflev = add_dummy_variables('region', reference_level=0)\nfeatures = ['household_income', 'household_size', 'couponper1', 'married'] + region_dummys\n\nReference category (dummy omitted): Central\n\n\n\nassembler = VectorAssembler(\n    inputCols = features,\n    outputCol='features')\n\ndtrain1 = assembler.transform(dtrain)\ndtest1  = assembler.transform(dtest)\n\nmodel = LinearRegression(\n    featuresCol='features',\n    labelCol='priceper1'\n).fit(dtrain1)\n\ndtest1 = model.transform(dtest1)\n\nprint(regression_table(model, assembler))\n\n+------------------------+--------+------+------------+---------+--------------+--------------+\n| Metric                 |  Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+------------------------+--------+------+------------+---------+--------------+--------------+\n| Beta: household_income | -0.060 | ***  |      0.006 |   0.000 |       -0.072 |       -0.048 |\n| Beta: household_size   | -0.032 | ***  |      0.005 |   0.000 |       -0.043 |       -0.022 |\n| Beta: couponper1       |  0.108 | ***  |      0.014 |   0.000 |        0.082 |        0.135 |\n| Beta: married          | -0.076 | ***  |      0.016 |   0.000 |       -0.109 |       -0.044 |\n| Beta: region_East      |  0.139 | ***  |      0.015 |   0.000 |        0.110 |        0.169 |\n| Beta: region_South     | -0.018 |      |      0.016 |   0.243 |       -0.048 |        0.013 |\n| Beta: region_West      |  0.048 | ***  |      0.014 |   0.002 |        0.020 |        0.076 |\n| Intercept              |  3.325 | ***  |      0.006 |   0.000 |        3.314 |        3.337 |\n-----------------------------------------------------------------------------------------------\n| Observations           | 14,653 |      |            |         |              |              |\n| R²                     |  0.046 |      |            |         |              |              |\n| RMSE                   |  0.653 |      |            |         |              |              |\n+------------------------+--------+------+------------+---------+--------------+--------------+\n\n\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nres = dtest1.select([\"prediction\", \"priceper1\"]).toPandas()\nres['residual'] = res['priceper1'] - res['prediction']\n\n\n\nplt.scatter(res[\"prediction\"], res[\"residual\"], alpha=0.2, color=\"darkgray\")\n\nsmoothed = sm.nonparametric.lowess(res[\"residual\"], res[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\nplt.xlabel(\"Predicted log(priceper1_\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nConclusion\n\nAs is shown in both the R2 and Residual Plot, this model is not a very accurate one to use. More features and interactions would have to be tried to find a better perfomring model for this data."
  },
  {
    "objectID": "danl_310_mats/danl_mapdata/danl_310_mapdata.html",
    "href": "danl_310_mats/danl_mapdata/danl_310_mapdata.html",
    "title": "DANL 310 | Geospatial Visualizations",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(skimr)\nlibrary(ggthemes)\nlibrary(socviz)\nlibrary(geofacet)\n\nWarning: package 'geofacet' was built under R version 4.4.3\n\n\n\nAttaching package: 'geofacet'\n\nThe following object is masked from 'package:socviz':\n\n    election\n\nlibrary(RColorBrewer)\nlibrary(maps)\n\nWarning: package 'maps' was built under R version 4.4.3\n\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map"
  },
  {
    "objectID": "danl_310_mats/danl_mapdata/danl_310_mapdata.html#draw-maps-u.s.-state-level-data",
    "href": "danl_310_mats/danl_mapdata/danl_310_mapdata.html#draw-maps-u.s.-state-level-data",
    "title": "DANL 310 | Geospatial Visualizations",
    "section": "Draw Maps (U.S. state-level data)",
    "text": "Draw Maps (U.S. state-level data)\n\nus_states &lt;- map_data(\"state\") # from the 'maps' package\np &lt;- ggplot(data = us_states,\n            mapping = aes(x = long, y = lat,\n                          group = group)) # wont show right w/o group = group\n\np + geom_polygon(fill = \"white\", color = \"black\")"
  },
  {
    "objectID": "danl_310_mats/danl_mapdata/danl_310_mapdata.html#part-1",
    "href": "danl_310_mats/danl_mapdata/danl_310_mapdata.html#part-1",
    "title": "DANL 310 | Geospatial Visualizations",
    "section": "Part 1",
    "text": "Part 1\n\nclimate_opinion_long &lt;- read_csv(\n  'https://bcdanl.github.io/data/climate_opinion_2021.csv')\n\nRows: 6284 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): GeoName, belief\ndbl (2): id, perc\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nQ1\n\ncopl &lt;- climate_opinion_long %&gt;% \n  filter(belief == 'human')\n\n\n\nQ2\n\ncm &lt;- socviz::county_map\n\ncm$id = as.integer(cm$id)\ncopl$id = as.integer(copl$id)\n\ncm_copl &lt;- left_join(cm, copl) # couldn't join b/c dtype mismatch\n\nJoining with `by = join_by(id)`\n\n\n\n\nQ3\n\nbrk = as.vector(\n  round(\n    c(min(cm_copl$perc, na.rm = T),\n      quantile(cm_copl$perc, .25, na.rm = T),\n      median(cm_copl$perc, na.rm = T),\n      quantile(cm_copl$perc, .75, na.rm = T),\n      max(cm_copl$perc, na.rm = T)\n      )\n    , 2\n    )\n  )\n\nlab = c(\n  paste0(brk[1], '\\n (min)'),\n  paste0(brk[2], '\\n (25th)'),\n  paste0(brk[3], '\\n (50th)'),\n  paste0(brk[4], '\\n (75th)'),\n  paste0(brk[5], '\\n (max)')\n)\n\n\nggplot(data = cm_copl, mapping = \n         aes(x = long, y = lat, fill = perc, group = group))+\n  geom_polygon()+\n    theme_map()+\n  scale_fill_gradient2(low = '#2E74C0',\n                       mid = 'white',\n                       high = '#CB454A',\n                       midpoint = median(cm_copl$perc, na.rm = T),\n                       breaks = brk,\n                       label = lab)+\n  guides(fill = guide_colorbar(barwidth = 30))+\n  theme(legend.position = 'bottom',\n        legend.justification = 'center',\n        legend.title = element_text(margin = margin(b = 18, r = 5)))+\n  labs(fill = 'Percent\\nBelief', title = 'U.S. Climate Opinion 2021')"
  },
  {
    "objectID": "danl_310_mats/danl_mapdata/danl_310_mapdata.html#part-2",
    "href": "danl_310_mats/danl_mapdata/danl_310_mapdata.html#part-2",
    "title": "DANL 310 | Geospatial Visualizations",
    "section": "Part 2",
    "text": "Part 2\n\nunemp_house_prices &lt;- read_csv(\n  'https://bcdanl.github.io/data/unemp_house_prices.csv')\n\nRows: 25704 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): state\ndbl  (3): unemploy_perc, house_price_index, house_price_perc\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlibrary(lubridate)\n\np2 &lt;- unemp_house_prices %&gt;% \n  filter(year(date) &gt;= 2008) %&gt;% \n  mutate(up = unemploy_perc / 100)\n\nadjust_labels &lt;- as_labeller(\n  function(x) {\n    case_when(\n      x == \"New Hampshire\" ~ \"N. Hampshire\",\n      x == \"District of Columbia\" ~ \"DC\",\n      TRUE ~ x\n    )\n  }\n)\n\nggplot(data = p2, aes(x = date, y = up))+\n  geom_area(fill = 'lightblue', color = 'black', alpha = 0.5)+\n  facet_geo(~state, labeller = adjust_labels)+\n  theme_minimal()+\n  scale_y_continuous(labels = scales::percent)+\n  scale_x_date(\n    breaks = c(ymd(\"2009-01-01\"), ymd(\"2011-01-01\"),ymd(\"2013-01-01\"),ymd(\"2015-01-01\"),ymd(\"2017-01-01\")),\n    labels = c(\"'09\", \"'11\", \"'13\", \"'15\", \"'17\")\n  )+\n  labs(x = '', y = 'Unemployment Rate')"
  },
  {
    "objectID": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html",
    "href": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html",
    "title": "DANL 310 | Adding Labels and Making Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nby_country &lt;- socviz::organdata |&gt; \n  group_by(consent_law, country) |&gt;\n  summarize(donors_mean= mean(donors, na.rm = TRUE),\n            donors_sd = sd(donors, na.rm = TRUE),\n            gdp_mean = mean(gdp, na.rm = TRUE),\n            health_mean = mean(health, na.rm = TRUE),\n            roads_mean = mean(roads, na.rm = TRUE),\n            cerebvas_mean = mean(cerebvas, na.rm = TRUE))\n\n`summarise()` has grouped output by 'consent_law'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#adding-text",
    "href": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#adding-text",
    "title": "DANL 310 | Adding Labels and Making Notes",
    "section": "Adding text",
    "text": "Adding text\n\norgandata &lt;-socviz::organdata\np &lt;- ggplot(data = organdata, \n            mapping = \n              aes(x = roads, \n                  y = donors))\np + geom_point() + \n  annotate(geom = \"text\", \n           x = 91, y = 33,# Sepcify using x and y scales in particular plot\n           label = \"A surprisingly high \\n recovery rate.\", # \\n is a line break\n           hjust = 0) # Right alignment\n\nWarning: Removed 34 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#adding-shape-rectangle",
    "href": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#adding-shape-rectangle",
    "title": "DANL 310 | Adding Labels and Making Notes",
    "section": "Adding shape (rectangle)",
    "text": "Adding shape (rectangle)\n\np &lt;- ggplot(data = organdata,\n            mapping = aes(x = roads, y = donors))\np + geom_point() +\n    annotate(geom = \"rect\", \n             xmin = 125, xmax = 155,\n             ymin = 30, ymax = 35, \n             fill = \"red\", \n             alpha = 0.2) + \n    annotate(geom = \"text\", \n             x = 157, y = 33,\n             label = \"A surprisingly high \\n recovery rate.\", \n             hjust = 0)\n\nWarning: Removed 34 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#adding-highlighting-behind-points",
    "href": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#adding-highlighting-behind-points",
    "title": "DANL 310 | Adding Labels and Making Notes",
    "section": "Adding highlighting behind points",
    "text": "Adding highlighting behind points\n\np &lt;- ggplot(mpg, aes(displ, hwy)) +\n  geom_point(\n    data = \n      filter(mpg, \n             manufacturer == \"subaru\"), \n    color = \"orange\", \n    size = 3) +\n  geom_point() \np + \n  annotate(geom = \"point\", \n           x = 5.5, y = 40, \n           colour = \"orange\", # Adds larger orange point\n           size = 3) + \n  annotate(geom = \"point\", \n           x = 5.5, y = 40) +  # Adds smaller black point to same loc.\n  annotate(geom = \"text\", \n           x = 5.6, y = 40, \n           label = \"subaru\", \n           hjust = 0)"
  },
  {
    "objectID": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#adding-arrow-to-point",
    "href": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#adding-arrow-to-point",
    "title": "DANL 310 | Adding Labels and Making Notes",
    "section": "Adding arrow to point",
    "text": "Adding arrow to point\n\np + \n  annotate(\n    geom = \"curve\", \n    x = 4, y = 35, \n    xend = 2.65, yend = 27, \n    curvature = 0.2, \n    arrow = \n      arrow(length = unit(2, \"mm\")) # Puts arrow on end of curved line\n  ) +\n  annotate(geom = \"text\", \n           x = 4.1, y = 35, \n           label = \"subaru\", \n           hjust = \"left\")"
  },
  {
    "objectID": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#q3",
    "href": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#q3",
    "title": "DANL 310 | Adding Labels and Making Notes",
    "section": "Q3",
    "text": "Q3\n\nmtcars &lt;- datasets::mtcars\nmtcars &lt;- mtcars %&gt;%   # A native pipe (|&gt;) does not work here.\n  mutate(car = rownames(.))\nrownames(mtcars) &lt;- 1:nrow(mtcars)\nDT::datatable(mtcars)\n\n\n\n\n\n\nggplot(data = mtcars, mapping = \n         aes(x = wt, y = mpg))+\n  geom_point(color = 'red')+\n  geom_text_repel(data = filter(mtcars, wt &gt; 5), mapping = aes(label = car))"
  },
  {
    "objectID": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#q1",
    "href": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#q1",
    "title": "DANL 310 | Adding Labels and Making Notes",
    "section": "Q1",
    "text": "Q1\n\nQ1a and Q1b\n\ngm &lt;- gapminder::gapminder\n\ngm &lt;- gm %&gt;% \n  filter((year == 2007) & (continent != 'Oceania') ) %&gt;% \n  mutate(country = fct_reorder(country, lifeExp))\n\np &lt;- ggplot(data = gm, mapping = \n         aes(y = country,\n             x = lifeExp))\np +  geom_point(color = '#0072B2', size = 4, shape = 20)+\n  facet_wrap(~continent, scales = 'free_y')+\n  geom_text(aes(label = lifeExp), hjust = -.25)+\n  labs(x = '', y = '', title = 'Life expectancy in 2007')+\n  theme_minimal() + # Things are cut off, not a good plot\n  annotate(geom = 'text',x = 50, y = 'Swaziland',label = 'Lowest Life Expectancy Overall',hjust =0)"
  },
  {
    "objectID": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#q2",
    "href": "danl_310_mats/danl_lec_addlabelsandnotes/danl_310_labelsandnotes.html#q2",
    "title": "DANL 310 | Adding Labels and Making Notes",
    "section": "Q2",
    "text": "Q2\n\nQ2a\n\nel &lt;- read_csv(\n  'https://bcdanl.github.io/data/electricity-usa-chn.csv')\n\nRows: 360 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): energy, label, iso3c\ndbl (2): year, value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(el, aes(x = year, y = value, color = energy))+\n  geom_line()+\n  facet_wrap(~iso3c)+\n  scale_color_viridis_d(option = 'B')+\n  theme(legend.position = 'top') # Unsure of rest\n\n\n\n\n\n\n\n\n\n\nQ2b\n\nel_b &lt;- el %&gt;% \n  group_by(iso3c, year) %&gt;% \n  mutate(tot = sum(value),\n         pct_val = value/tot)\n\nggplot(el_b, aes(x = year, y = pct_val, color = energy))+\n  geom_line()+\n  facet_wrap(~iso3c)+\n  scale_color_viridis_d(option = 'B')+\n  theme(legend.position = 'top')"
  },
  {
    "objectID": "danl_310_mats/danl_lec_3_4/danl_310_lec_3_and_4.html",
    "href": "danl_310_mats/danl_lec_3_4/danl_310_lec_3_and_4.html",
    "title": "DANL 310 | ggplot2 basics notes",
    "section": "",
    "text": "Packages & DataFrame\n\nlibrary(gapminder)\nlibrary(tidyverse)\nlibrary(skimr)\ngapminder &lt;- gapminder::gapminder\nskim(gapminder)\n\n\nData summary\n\n\nName\ngapminder\n\n\nNumber of rows\n1704\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncountry\n0\n1\nFALSE\n142\nAfg: 12, Alb: 12, Alg: 12, Ang: 12\n\n\ncontinent\n0\n1\nFALSE\n5\nAfr: 624, Asi: 396, Eur: 360, Ame: 300\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1979.50\n17.27\n1952.00\n1965.75\n1979.50\n1993.25\n2007.0\n▇▅▅▅▇\n\n\nlifeExp\n0\n1\n59.47\n12.92\n23.60\n48.20\n60.71\n70.85\n82.6\n▁▆▇▇▇\n\n\npop\n0\n1\n29601212.32\n106157896.74\n60011.00\n2793664.00\n7023595.50\n19585221.75\n1318683096.0\n▇▁▁▁▁\n\n\ngdpPercap\n0\n1\n7215.33\n9857.45\n241.17\n1202.06\n3531.85\n9325.46\n113523.1\n▇▁▁▁▁\n\n\n\n\nmpg &lt;- mpg\ndiamonds &lt;- diamonds\n\n\n\nMaking a Plot\n\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp))\np + \n  geom_point(alpha = 0.25, color = 'blue') + \n  geom_smooth(method = 'gam', color = 'maroon')\n\n\n\n\n\n\n\n\n\n\nPlot with color argument in aes() mapping\n\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp,\n                          color = continent))\np + \n  geom_point(alpha = 0.5) + \n  geom_smooth() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPlot with aesthetics set in individual geoms\n\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp))\np + \n  geom_point(color = 'purple') +\n  geom_smooth(color = 'orange', method = 'loess', se = F, size = 1.5) # different non-linear method, and shading for error false\n\n\n\n\n\n\n\n\n\n\nUsing scale_*() and labs() functions with plot\n\np &lt;- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp))\np + \n  geom_point(alpha = 0.3) +\n  geom_smooth() +\n  scale_x_log10(labels = scales::dollar) +\n  labs(x = \"GDP Per Capita\", y = \"Life Expectancy in Years\",\n        title = \"Economic Growth and Life Expectancy\",\n        subtitle = \"Data points are country-years\",\n        caption = \"Source: Gapminder.\")\n\n\n\n\n\n\n\n\n\n\nCan have different aes() mappings over different geoms\n\np &lt;- ggplot(data = gapminder, \n            mapping = aes(x = gdpPercap, y = lifeExp))\n\np + geom_point(mapping = aes(color = continent)) +\n    geom_smooth(method = \"loess\")  +\n    scale_x_continuous(trans = scales::log_trans())  # natural log\n\n\n\n\n\n\n\n\n\n\nHistogram and Freq Poly geoms with binwidth\n\nggplot(data = diamonds, mapping = aes(x = price)) +\n  geom_histogram(binwidth = 500, fill = 'navy') +\n  geom_freqpoly(color = 'maroon', linewidth = 1.25, binwidth = 500)\n\n\n\n\n\n\n\n\n\n\nFacet wrap with scales param\n\nggplot(data = diamonds, mapping = aes(x = price)) +\n  geom_histogram(binwidth = 30)+\n  facet_wrap(~cut, scales = 'free_y') # Allows for the y_scale to be free, in this case fair cut diamond price distribution is hard to see without free_y for scales\n\n\n\n\n\n\n\n\n\n\nSales df loading\n\nsale_df &lt;- read_csv(\n  \"https://bcdanl.github.io/data/home_sales_nyc.csv\")\n\n\n\nHistograms with log10(x)\n\nggplot(data = sale_df,\n       mapping = aes(x = sale_price))+\n  geom_histogram(binwidth = 30000, fill = 'steelblue')\n\n\n\n\n\n\n\nggplot(data = sale_df,\n       mapping = aes(x = log10(sale_price)))+\n  geom_histogram(bins = 200, fill = 'steelblue')\n\n\n\n\n\n\n\n\n\n\nBar geoms with stat = ‘identity’\n\nggplot(data = diamonds,\n       mapping = aes(x = cut, fill = cut))+\n  geom_bar()\n\n\n\n\n\n\n\nggplot(data = diamonds,\n       mapping = aes(x = cut, y = price))+\n  geom_bar(stat = 'identity')# Doesn't make sense to do here, but allows you to set your own y mapping to override count default, geom_col() can also be used which just allows you to set bar height with y in aes(), ggplot also used alphebetical order by default to map out categories, can factor with a given # of levels to specify order, ie; \"Fair\"&lt;\"Good\"&lt;\"Ideal\"&lt;\"Premium\"&lt;\"Very Good\"\n\n\n\n\n\n\n\n\n\n\nBar geoms with proportion instead of count\n\nggplot(data = diamonds, \n       mapping = aes(x = cut,\n                     y = after_stat(prop), \n                     group = 1.75))+\n  geom_bar()# have to also put group = some number inside of aes() in order for it to calc proportions successfully\n\n\n\n\n\n\n\n\n\n\nStat summary\n\nggplot(data = diamonds)+\n  stat_summary(\n    mapping = aes(x = cut, y = depth),\n    fun.min = min, # If don't include these, will just be dots representing whatever is after fun =\n    fun.max = max,\n    fun = median # Can also put mean instead of median\n  )\n\n\n\n\n\n\n\n\n\n\nColor and fill asthetic (position adjustment, primarily bar charts)\n\nggplot(data = diamonds)+\n  geom_bar(mapping = \n             aes(x = cut,\n                 fill = cut # color will outline bars, fill will fill colors into bars,\n                 ))+ # all done by var set to \n  guides(fill = \"none\")# guides can remove legend, and can alter formatting for legends and theme stuff\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds)+\n  geom_bar(mapping = \n             aes(\n               x = cut,\n               fill = clarity\n             ))\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds)+\n  geom_bar(mapping = \n             aes(\n               x = cut,\n               fill = clarity\n             ), position = 'dodge') # If don't include 'dodge' bars will be stacked, if do will have multiple bars per x-axis category, clustered bar chart\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds)+\n  geom_bar(mapping = \n             aes(\n               x = cut,\n               fill = clarity\n             ), position = 'fill')+\n  labs(y = 'proportion')# Shows proportion of each clarity value per each cut value\n\n\n\n\n\n\n\n\n\n\nCoordinate Systems\n\nggplot(data = mpg,\n       mapping =\n         aes(x = cty,\n             y = hwy))+\n  geom_point()+\n  geom_abline()\n\n\n\n\n\n\n\n\n\nggplot(data = mpg,\n       mapping =\n         aes(x = cty,\n             y = hwy))+\n  geom_point()+\n  geom_abline()+\n  coord_fixed()"
  },
  {
    "objectID": "danl_310_mats/danl_hw_1/html_document.html",
    "href": "danl_310_mats/danl_hw_1/html_document.html",
    "title": "ggplot Basics | Hw 1 Post",
    "section": "",
    "text": "library(tidyverse)\ndf &lt;- datasets::mtcars\n\ndf &lt;- df %&gt;% \n  filter(cyl %in% c(4,6)) %&gt;% # Say you wanted to only visualize relationships for 4 or 6 cylinder.\n  mutate(cyl_chr = as.character(cyl))\n\ndf2 &lt;- male_Aus &lt;- read_csv(\n  'https://bcdanl.github.io/data/aus_athletics_male.csv')\n\ndf2 &lt;-df2 %&gt;% \n  group_by(sport) %&gt;% \n  summarise(mean_bf = mean(pcBfat)) %&gt;% \n  mutate(sport = fct_reorder(sport, mean_bf))"
  },
  {
    "objectID": "danl_310_mats/danl_hw_1/html_document.html#if-needed-apply-dplyr-transformation-functions-to-make-it-so-the-target-distribution-or-relationship-can-be-obtained",
    "href": "danl_310_mats/danl_hw_1/html_document.html#if-needed-apply-dplyr-transformation-functions-to-make-it-so-the-target-distribution-or-relationship-can-be-obtained",
    "title": "ggplot Basics | Hw 1 Post",
    "section": "",
    "text": "library(tidyverse)\ndf &lt;- datasets::mtcars\n\ndf &lt;- df %&gt;% \n  filter(cyl %in% c(4,6)) %&gt;% # Say you wanted to only visualize relationships for 4 or 6 cylinder.\n  mutate(cyl_chr = as.character(cyl))\n\ndf2 &lt;- male_Aus &lt;- read_csv(\n  'https://bcdanl.github.io/data/aus_athletics_male.csv')\n\ndf2 &lt;-df2 %&gt;% \n  group_by(sport) %&gt;% \n  summarise(mean_bf = mean(pcBfat)) %&gt;% \n  mutate(sport = fct_reorder(sport, mean_bf))"
  },
  {
    "objectID": "danl_310_mats/danl_hw_1/html_document.html#create-ggplot-object-with-any-mappings-and-assign-df",
    "href": "danl_310_mats/danl_hw_1/html_document.html#create-ggplot-object-with-any-mappings-and-assign-df",
    "title": "ggplot Basics | Hw 1 Post",
    "section": "Create ggplot() object with any mappings and assign df",
    "text": "Create ggplot() object with any mappings and assign df\n\np = ggplot(data = df, mapping = \n         aes(x = mpg, y = hp))\n\np2 = ggplot(data = df2, mapping = \n              aes(x = sport, y = mean_bf))"
  },
  {
    "objectID": "danl_310_mats/danl_hw_1/html_document.html#apply-the-applicable-geoms-and-theme-or-labs-modifications",
    "href": "danl_310_mats/danl_hw_1/html_document.html#apply-the-applicable-geoms-and-theme-or-labs-modifications",
    "title": "ggplot Basics | Hw 1 Post",
    "section": "Apply the applicable geoms and theme or labs modifications",
    "text": "Apply the applicable geoms and theme or labs modifications\n\nGeoms in ggplot2 allow for one to choose which plotting methods are performed and calibrate certain parameters.\n\n\np + geom_point(aes(color = cyl_chr), size = 2, shape = 16)+\n  labs(x = 'Miles per Gallon', y = 'Horsepower', color = 'Cylinders', title = 'Miles Per Gallon vs. Horsepower')+\n  theme_bw()+\n  theme(legend.position = c(0.95, 0.9))+\n  scale_color_manual(values = c('red',  'blue'))\n\n\n\n\n\n\n\n\n\np2 + geom_bar(aes(fill = sport), position = 'dodge', stat = 'identity')+\n  guides(fill = F)+\n  theme_bw()+\n  labs(x = 'Sport', y = 'Mean Body Fat', title = 'Mean Body Fat vs. Sport')"
  },
  {
    "objectID": "class_projects.html",
    "href": "class_projects.html",
    "title": "Class Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "class_blog.html",
    "href": "class_blog.html",
    "title": "Class Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWelcome All!\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "class_blog_posts/welcome/index.html",
    "href": "class_blog_posts/welcome/index.html",
    "title": "Welcome All!",
    "section": "",
    "text": "Welcome to my blog powered by Quarto and GitHub!!\n\nAs a Data Analytics major, it is helpful to hone my abilities by doing exploratory data analysis. With that, I will be posting little projects here and there on my website, feel free to take a look!"
  },
  {
    "objectID": "danl_310.html",
    "href": "danl_310.html",
    "title": "DANL 310",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDANL 310 | Geospatial Visualizations\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\nDaniel Noone\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nBen & Jerry’s | Hw 2 Post\n\n\nA descriptive and visual analysis of a Ben & Jerry’s data set\n\n\n\n\n\nMar 23, 2025\n\n\nDaniel Noone\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nDANL 310 | Refining Plots\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nDaniel Noone\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nDANL 310 | Adding Labels and Making Notes\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nDaniel Noone\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nggplot Basics | Hw 1 Post\n\n\nIn this post the basics of ggplot2 syntax with be discussed\n\n\n\n\n\nFeb 13, 2025\n\n\nDaniel Noone\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nDANL | Lec. Notes\n\n\n\n\n\n\n\n\nFeb 12, 2025\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nDANL 310 | ggplot2 basics notes\n\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\nDaniel Noone\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl_310_mats/danl_hw_2/html_document.html",
    "href": "danl_310_mats/danl_hw_2/html_document.html",
    "title": "Ben & Jerry’s | Hw 2 Post",
    "section": "",
    "text": "Loading Data\n\nlibrary(tidyverse)\nlibrary(skimr)\nice_cream &lt;- read_csv('https://bcdanl.github.io/data/ben-and-jerry-cleaned.csv')\nskim(ice_cream)\n\n\nData summary\n\n\nName\nice_cream\n\n\nNumber of rows\n21974\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nlogical\n8\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nflavor_descr\n0\n1\n3\n30\n0\n50\n0\n\n\nsize1_descr\n0\n1\n9\n9\n0\n2\n0\n\n\nregion\n0\n1\n4\n7\n0\n4\n0\n\n\nrace\n0\n1\n5\n5\n0\n4\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nusecoup\n0\n1\n0.11\nFAL: 19629, TRU: 2345\n\n\nmarried\n0\n1\n0.60\nTRU: 13276, FAL: 8698\n\n\nhispanic_origin\n0\n1\n0.05\nFAL: 20919, TRU: 1055\n\n\nmicrowave\n0\n1\n0.98\nTRU: 21567, FAL: 407\n\n\ndishwasher\n0\n1\n0.77\nTRU: 16983, FAL: 4991\n\n\nsfh\n0\n1\n0.73\nTRU: 16076, FAL: 5898\n\n\ninternet\n0\n1\n0.84\nTRU: 18529, FAL: 3445\n\n\ntvcable\n34\n1\n0.64\nTRU: 13954, FAL: 7986\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npriceper1\n0\n1\n3.31\n0.67\n0\n3\n3.34\n3.59\n9.48\n▁▇▂▁▁\n\n\nhousehold_id\n0\n1\n16612005.04\n11685954.46\n2000358\n8142253\n8401573.00\n30183891.00\n30440689.00\n▂▇▁▁▇\n\n\nhousehold_income\n0\n1\n125290.80\n57188.36\n40000\n80000\n110000.00\n170000.00\n310000.00\n▇▃▅▂▁\n\n\nhousehold_size\n0\n1\n2.46\n1.34\n1\n2\n2.00\n3.00\n9.00\n▇▃▁▁▁\n\n\ncouponper1\n0\n1\n0.13\n0.52\n0\n0\n0.00\n0.00\n8.98\n▇▁▁▁▁\n\n\n\n\n\n\n\nData Analysis\n\nlibrary(RColorBrewer)\nlibrary(ggtext)\nlibrary(ggrepel)\nlibrary(hrbrthemes)\nlibrary(ggthemes)\n\n\nviz1 &lt;- ice_cream %&gt;% \n  group_by(region, married, race) %&gt;% \n  summarize(hh_inc = mean(household_income)) %&gt;% \n  mutate(region = fct_reorder(region, hh_inc),\n         race = fct_reorder(race, hh_inc),\n         married = ifelse(married == F, 'Not Married', 'Married'),\n         hh_inc = hh_inc / 1000)\n\nggplot(viz1, mapping =\n         aes(y = region, x = hh_inc, fill = race))+\n  geom_bar(stat = 'identity', position = position_dodge(width = 0.7), width = 0.6)+\n  theme_ipsum(base_family = 'sans')+\n  facet_wrap(~married)+\n  labs(x = 'Mean Household Income (1000s of dollars)', y = 'Region', title = 'Mean Household Income', subtitle = 'Bar chart depicts mean household income by race and marriage status')+\n  scale_fill_brewer(palette = 'Dark2')+\n  theme(plot.title = element_text(color = 'darkblue', face = 'bold'),\n        plot.subtitle = element_text(face = 'bold'),\n        plot.background = element_rect(fill = 'lightblue'))\n\n\n\n\n\n\n\n\n\nviz2 &lt;- ice_cream %&gt;%\n  filter(couponper1 &gt; 0)\n\nggplot(viz2, mapping =\n         aes(x = priceper1, y = couponper1))+\n  geom_point()+\n  geom_smooth(color = 'red', se = F)+\n  facet_wrap(~region)+\n  theme_ipsum(base_family = 'sans')+\n  labs(x = 'Price per Unit (USD)', y = 'Coupon Discount Amount (%)', title = 'Unit Price vs. Discount Amount per Region', subtitle = 'Faceted scatterplot depicts the relationship between per unit price and coupon discount amount, by region.')+\n  scale_fill_brewer(palette = 'Dark2')+\n  theme(plot.title = element_text(color = 'darkblue', face = 'bold'),\n        plot.subtitle = element_text(face = 'bold'),\n        plot.background = element_rect(fill = 'lightblue'))"
  },
  {
    "objectID": "danl_310_mats/danl_lec_5_6/danl_310_lec_5_and_6.html",
    "href": "danl_310_mats/danl_lec_5_6/danl_310_lec_5_and_6.html",
    "title": "DANL | Lec. Notes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ngsm &lt;- socviz::gss_sm"
  },
  {
    "objectID": "danl_310_mats/danl_lec_5_6/danl_310_lec_5_and_6.html#q2",
    "href": "danl_310_mats/danl_lec_5_6/danl_310_lec_5_and_6.html#q2",
    "title": "DANL | Lec. Notes",
    "section": "Q2",
    "text": "Q2\n\nod &lt;- socviz::organdata\n\n\nQ2a (Cleveland Dotplots, sometimes preferred to barcharts)\n\na &lt;- od %&gt;% \n  group_by(consent_law, country) %&gt;% \n  summarise(dpr_mean = mean(donors, na.rm = T))\n\n`summarise()` has grouped output by 'consent_law'. You can override using the\n`.groups` argument.\n\nggplot(a, mapping = aes(x = dpr_mean, y = fct_reorder(country, dpr_mean), colour = consent_law))+\n  geom_point()+\n  theme(legend.position = 'top')+\n  labs(x = 'Donor Procurement Rate', y = '')\n\n\n\n\n\n\n\n\n\n\nQ2b\n\nb &lt;- a\n\nggplot(b, mapping = aes(x = dpr_mean, y = fct_reorder(country, dpr_mean, na.rm = T)))+\n  geom_point()+\n  facet_wrap(~consent_law, ncol = 1, scales = 'free_y')+\n  labs(y = '', x = 'Donor Procurement Rate')\n\n\n\n\n\n\n\n\n\n\nQ2c (dot and whisker plot, gives error bars for st dev)"
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "Should choose color palette based on type of data plotting\n\nex) Unordered vars need distinct colors, ordered vars need graded color scheme which changes as values change\n\nChoose palettes for mappings thru scale_ functions added to plot for color or fill\n\n\n\n\n# RColorBrewer provides a wide variety of named palettes\n # Access using scale_color_brewer() or scale_fill_brewer() with pallete = param\nlibrary(RColorBrewer)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(hrbrthemes)\nlibrary(ggthemes)\n\n  \n\n\n\norgandata = socviz::organdata\np &lt;- ggplot(data = organdata,\n            mapping = \n              aes(x = roads, \n                  y = donors, \n                  color = world))\n\np + geom_point(size = 2) + \n  scale_color_brewer(\n    palette = \"Set2\") +\n  theme(legend.position = \"top\")\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\np + geom_point(size = 2) + \n  scale_color_brewer(\n    palette = \"Pastel2\") +\n  theme(legend.position = \"top\")\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\np + geom_point(size = 2) + \n  scale_color_brewer(\n    palette = \"Dark2\") +\n  theme(legend.position = \"top\")\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan specify color palettes manually using scale_color_manual() or scale_fill_manual()\nwithin a c(), specify colors using ‘color name’ or hex values (can access hex values here\n\n\n\n\np + geom_point(size = 2) + \n  scale_color_manual(\n    values = c(\"#3c6ff8\", \"#afd68d\", \n               \"#8467ad\", \"#82857f\")) +\n  theme_ipsum() + \n  theme(legend.position = \"top\") \n\nWarning: Removed 34 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow action shows the palettes in RColorBrewer\nAlong with a bool column showing if the palette is colorblind friendly or not\n\n\nbrewer.pal.info\n\n         maxcolors category colorblind\nBrBG            11      div       TRUE\nPiYG            11      div       TRUE\nPRGn            11      div       TRUE\nPuOr            11      div       TRUE\nRdBu            11      div       TRUE\nRdGy            11      div      FALSE\nRdYlBu          11      div       TRUE\nRdYlGn          11      div      FALSE\nSpectral        11      div      FALSE\nAccent           8     qual      FALSE\nDark2            8     qual       TRUE\nPaired          12     qual       TRUE\nPastel1          9     qual      FALSE\nPastel2          8     qual      FALSE\nSet1             9     qual      FALSE\nSet2             8     qual       TRUE\nSet3            12     qual      FALSE\nBlues            9      seq       TRUE\nBuGn             9      seq       TRUE\nBuPu             9      seq       TRUE\nGnBu             9      seq       TRUE\nGreens           9      seq       TRUE\nGreys            9      seq       TRUE\nOranges          9      seq       TRUE\nOrRd             9      seq       TRUE\nPuBu             9      seq       TRUE\nPuBuGn           9      seq       TRUE\nPuRd             9      seq       TRUE\nPurples          9      seq       TRUE\nRdPu             9      seq       TRUE\nReds             9      seq       TRUE\nYlGn             9      seq       TRUE\nYlGnBu           9      seq       TRUE\nYlOrBr           9      seq       TRUE\nYlOrRd           9      seq       TRUE\n\n\n\n\n\n\nSometimes want to use color to highlight some aspect of data\n\n\ncounty_data &lt;- socviz::county_data\n\n\n# DEM Blue and REP Red\nparty_colors &lt;- \n  c(\"#2E74C0\", \"#CB454A\") \n\np0 &lt;- ggplot(\n  data = filter(county_data, \n                flipped == \"No\"),\n  mapping = \n    aes(x = pop, \n        y = black/100) )\n\np1 &lt;- p0 + \n  geom_point(alpha = 0.15, \n             color = \"gray50\") \np1\n\n\n\n\n\n\n\n\n–&gt; Looks very skewed with the normal scale, so use a log10 scale instead\n\n p0 &lt;- ggplot(\n  data = filter(county_data, \n                flipped == \"No\"),\n  mapping = \n    aes(x = pop, \n        y = black/100) )\n\np1 &lt;- p0 + \n  geom_point(alpha = 0.15, \n             color = \"gray50\")+\n  scale_x_log10(labels=scales::comma)\n\np1\n\n\n\n\n\n\n\n\n–&gt; Now the data is more normally distributed, looks better as a viz, the last one was flipped == ‘No’, next will be flipped == ‘Yes’\n\np2 &lt;- p1 + \n  geom_point(\n    data = filter(county_data,\n                  flipped == \"Yes\"),\n    mapping = \n      aes(x = pop, y = black/100,\n          color = partywinner16)) +\n    scale_color_manual(\n      values = party_colors)\np2\n\n\n\n\n\n\n\n\n–&gt; viz will now be cleaned up and refined label-wise, and y-axis –&gt; % labels\n\np3 &lt;- p2 + \n  scale_y_continuous(\n    labels=scales::percent) +\n  labs(\n    color = \n      \"County flipped to ... \",\n    x = \n      \"County Population (log scale)\",\n    y = \n      \"Percent Black Population\",\n    title = \n      \"Flipped counties, 2016\",\n    caption = \n      \"Counties in gray did not flip.\")\np3\n\n\n\n\n\n\n\n\n–&gt; Next viz now labels state on points that were flipped and have % AA &gt; 25\n\nlibrary(ggrepel)\np4 &lt;- p3 + \n  geom_text_repel(\n    data = \n      filter(\n       county_data,\n       flipped == \"Yes\" & black &gt;25),\n    mapping = \n      aes(x = pop, y = black/100,\n          label = state), \n    size = 2)\n\np4 + theme_minimal() + \n  theme(legend.position=\"top\")\n\n\n\n\n\n\n\n\n\n\n\n\np4 + theme_economist() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\np4 + theme_wsj() +\n  theme(\n    plot.title = \n      element_text(size = rel(0.6)), # rel() mean relative size, here sets title 60% of original size\n    legend.title = \n      element_text(size = rel(0.35)),\n    plot.caption = \n      element_text(size = rel(0.35)),\n    legend.position = \"top\")\n\n\n\n\n\n\n\n\n\np4 + theme(\n  legend.position = \"top\",\n  plot.title = element_text(\n      size = rel(2),\n      lineheight = .5,\n      family = \"Times\", # fonts not found, but this is how to set font type\n      face = \"bold.italic\", # how to set bolt/italic or both\n      color = \"orange\"),\n  axis.text.x = element_text(\n      size = rel(1.1),\n      family = \"Courier\",\n      face = \"bold\",\n      color = \"purple\")\n  )\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n–&gt; theme() allows us to have fine control over appearance of our visualizations\n\np4 + theme(\n  legend.position = \"top\",\n  plot.title = element_text(\n    size = rel(2),\n    lineheight = .5,\n    family = \"Times\",\n    face = \"bold.italic\",\n    colour = \"orange\"),\n  axis.text.x = element_blank()\n  ) \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n–&gt; element_blank() removes an element, as seen above with the x-axis labels\n\n\n\nShowing age distributiin of General Social Survey (GSS) respondents over the years\n\n\ngss_lon &lt;- socviz::gss_lon\n\n\n\n\nyrs &lt;- c(seq(1972, 1988, 4), \n         1993, \n         seq(1996, 2016, 4))\n\nmean_age &lt;- gss_lon |&gt;\n    filter( !is.na(age), \n            year %in% yrs) |&gt;\n    group_by(year) |&gt;\n    summarize(\n      xbar = round(\n        mean(age, na.rm = TRUE), 0)\n      ) # Removing obs in age var with missing vals, only for the years in yrs above, calc mean age for each year\n\nmean_age$y &lt;- 0.3 \n\nyr_labs &lt;- data.frame(\n  x = 85, y = 0.8, \n  year = yrs)  # to position the age as a text label\n\n\np &lt;- ggplot(\n  data = \n    filter(gss_lon, year %in% yrs),\n  mapping = \n    aes(x = age))\n\np1 &lt;- p + \n  geom_density(\n    fill = \"black\", color = FALSE,\n    alpha = 0.9, \n    mapping = aes(y = ..scaled..))\np1\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\n\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\np2 &lt;- p1 + \n  geom_vline(\n    data = filter(\n      mean_age, year %in% yrs),\n    aes(xintercept = xbar), \n    color = \"white\", linewidth = 0.5) + \n  geom_text(\n    data = filter(mean_age, \n             year %in% yrs),\n    aes(x = xbar, y = y, label = xbar), \n    nudge_x = 7.5, color = \"white\", \n    size = 3.5, hjust = 1) +\n  geom_text(data = filter(\n    yr_labs, year %in% yrs),\n    aes(x = x, y = y, label = year)) \np2\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n–&gt; nudge_x arg pushes label slightly to right of it’s associated x-value\n\np3 &lt;- p2  + \n  facet_grid(year ~ ., switch = \"y\")\np3\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n–&gt; switch = arg in facet_grid() moves labels to the left\n\np2a &lt;- p3 + \n  theme(\n    plot.title = \n      element_text(size = 16),\n    axis.text.x= \n      element_text(size = 12),\n    axis.title.y=element_blank(),\n    axis.text.y=element_blank(),\n    axis.ticks.y = element_blank(),\n    strip.background = element_blank(),\n    strip.text.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()) +\n  labs(x = \"Age\", y = NULL,\n       title = \n         \"Age Distribution of\\nGSS Respondents\")\np2a\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# install.packages(\"ggridges\")\nlibrary(ggridges)\n\n\nAllows distributions to overlap vertically\n\nespecially useful for repeated distributional measures that change in clear direction\n\n\n\np &lt;- ggplot(\n  data = gss_lon,\n  mapping = \n    aes(x = age, \n        y = factor(year, \n                   levels = rev(unique(year)), \n                   ordered = TRUE)))\n\n–&gt; factor() convets var to a factor var with set levels - levels param allows to set categories of categorical var\n\np &lt;- ggplot(\n  data = \n    filter(gss_lon, year %in% yrs),\n  mapping = \n    aes(x = age))\n\np2b &lt;- p + \n  geom_density_ridges(\n    alpha = 0.6, fill = \"lightblue\", \n    scale = 1.5,\n    mapping = aes(y = factor(year, levels = rev(unique(year)), ordered = TRUE))) +  \n    scale_x_continuous(\n      breaks = c(25, 50, 75)) +\n    scale_y_discrete(\n      expand = c(0.01, 0)) + \n    labs(x = \"Age\", y = NULL, \n         title = \n           \"Age Distribution of\\nGSS Respondents\") +\n    theme_ridges() +  # make labels aligned properly\n    theme(\n      title = \n        element_text(\n          size = 16, face = \"bold\"))\np2b\n\nPicking joint bandwidth of 3.48\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n–&gt; expand arg in scale_y_discrete() asjusts scalling of y-axis slightly\n\n\n\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ngrid.arrange(p2a, p2b, nrow = 1)   # sub-figures\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nPicking joint bandwidth of 3.48\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n–&gt; essentially takes multiple figures and arranges them in a table\n\n\n\n\nUsing df studebt to show distribution of debt pct and how it varies by type\n\n\nstudebt &lt;- socviz::studebt\n\np_xlab &lt;- \"Amount Owed, in thousands of Dollars\"\np_title &lt;- \"Outstanding Student Loans\"\np_subtitle &lt;- \"44 million borrowers owe a total of $1.3 trillion\"\np_caption &lt;- \"Source: FRB NY\"\n\nf_labs &lt;- c(`Borrowers` = \"Percent of\\nall Borrowers\",\n            `Balances` = \"Percent of\\nall Balances\")\n\n\np &lt;- ggplot(\n  data = studebt,\n  mapping = \n    aes(x = pct/100, y = Debt,\n        fill = type))\np1 &lt;- p + geom_col()\np1\n\n\n\n\n\n\n\n\n\np2 &lt;- p1 +\n  scale_fill_brewer(\n    type = \"qual\", palette = \"Dark2\")\np2\n\n\n\n\n\n\n\n\n\np3 &lt;- p2 +\n  scale_x_continuous(\n    labels = scales::percent)\np3\n\n\n\n\n\n\n\n\n\np4 &lt;- p3 +\n  guides(fill = FALSE)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\np4\n\n\n\n\n\n\n\n\n–&gt; removing legend with guides(fill = FALSE)\n\np5 &lt;- p4 +\n  facet_grid(\n    .~ type, \n    labeller = as_labeller(f_labs))\np5\n\n\n\n\n\n\n\n\n–&gt; faceting here using custom labels as defined in f_labs at the beginning of this section\n\np6 &lt;- p5 +\n  labs(y = NULL, x = p_xlab, \n       caption = p_caption,\n       title = p_title,\n       subtitle = p_subtitle)\np6\n\n\n\n\n\n\n\n\n\np7 &lt;- p6 +\n  theme(strip.text.x = \n          element_text(face = \"bold\"))\np7\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of having separate bars distinguished by height, - can use 100% stacked bars (proportions of bar)\nCan then facet and lay bars on sides for comparison\n\n\np &lt;- ggplot(\n  studebt, \n  aes(x = pct/100, y = type, \n      fill = Debtrc))\np1 &lt;- p + \n  geom_col(color = \"gray80\")+\n  theme(legend.position = 'top')\np1\n\n\n\n\n\n\n\n\n\np2 &lt;- p1 +\n  scale_y_discrete(\n    labels = as_labeller(f_labs))\np2\n\n\n\n\n\n\n\n\n\np3 &lt;- p2 +\n  scale_x_continuous(\n    labels = scales::percent)\np3\n\n\n\n\n\n\n\n\n\np4 &lt;- p3 +\n  scale_fill_viridis_d(\n    option = \"B\")\np4\n\n\n\n\n\n\n\n\n\np5 &lt;- p4 +\n  guides(\n    fill = \n      guide_legend(\n        reverse = TRUE,\n        title.position = \"top\",\n        label.position = \"bottom\",\n        keywidth = 3,\n        nrow = 1))\np5\n\n\n\n\n\n\n\n\n\np6 &lt;- p5 +\n  labs(x = NULL, y = NULL,\n       fill = \"Amount Owed, in thousands of dollars\",\n       caption = p_caption,\n       title = p_title,\n       subtitle = p_subtitle)\np6\n\n\n\n\n\n\n\n\n\np7 &lt;- p6 +\n  theme(legend.position = \"top\",\n        axis.text.y = \n          element_text(\n            face = \"bold\",\n            hjust = 1, \n            size = 12),\n        axis.ticks.length = \n          unit(0, \"cm\"),\n        panel.grid.major.y = \n          element_blank())\np7"
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#colors-with-rcolorbrewer",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#colors-with-rcolorbrewer",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "# RColorBrewer provides a wide variety of named palettes\n # Access using scale_color_brewer() or scale_fill_brewer() with pallete = param\nlibrary(RColorBrewer)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(hrbrthemes)\nlibrary(ggthemes)\n\n  \n\n\n\norgandata = socviz::organdata\np &lt;- ggplot(data = organdata,\n            mapping = \n              aes(x = roads, \n                  y = donors, \n                  color = world))\n\np + geom_point(size = 2) + \n  scale_color_brewer(\n    palette = \"Set2\") +\n  theme(legend.position = \"top\")\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\np + geom_point(size = 2) + \n  scale_color_brewer(\n    palette = \"Pastel2\") +\n  theme(legend.position = \"top\")\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\np + geom_point(size = 2) + \n  scale_color_brewer(\n    palette = \"Dark2\") +\n  theme(legend.position = \"top\")\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#specifying-colors-manually",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#specifying-colors-manually",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "Can specify color palettes manually using scale_color_manual() or scale_fill_manual()\nwithin a c(), specify colors using ‘color name’ or hex values (can access hex values here\n\n\n\n\np + geom_point(size = 2) + \n  scale_color_manual(\n    values = c(\"#3c6ff8\", \"#afd68d\", \n               \"#8467ad\", \"#82857f\")) +\n  theme_ipsum() + \n  theme(legend.position = \"top\") \n\nWarning: Removed 34 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#colorblindness",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#colorblindness",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "Below action shows the palettes in RColorBrewer\nAlong with a bool column showing if the palette is colorblind friendly or not\n\n\nbrewer.pal.info\n\n         maxcolors category colorblind\nBrBG            11      div       TRUE\nPiYG            11      div       TRUE\nPRGn            11      div       TRUE\nPuOr            11      div       TRUE\nRdBu            11      div       TRUE\nRdGy            11      div      FALSE\nRdYlBu          11      div       TRUE\nRdYlGn          11      div      FALSE\nSpectral        11      div      FALSE\nAccent           8     qual      FALSE\nDark2            8     qual       TRUE\nPaired          12     qual       TRUE\nPastel1          9     qual      FALSE\nPastel2          8     qual      FALSE\nSet1             9     qual      FALSE\nSet2             8     qual       TRUE\nSet3            12     qual      FALSE\nBlues            9      seq       TRUE\nBuGn             9      seq       TRUE\nBuPu             9      seq       TRUE\nGnBu             9      seq       TRUE\nGreens           9      seq       TRUE\nGreys            9      seq       TRUE\nOranges          9      seq       TRUE\nOrRd             9      seq       TRUE\nPuBu             9      seq       TRUE\nPuBuGn           9      seq       TRUE\nPuRd             9      seq       TRUE\nPurples          9      seq       TRUE\nRdPu             9      seq       TRUE\nReds             9      seq       TRUE\nYlGn             9      seq       TRUE\nYlGnBu           9      seq       TRUE\nYlOrBr           9      seq       TRUE\nYlOrRd           9      seq       TRUE"
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#layering-color-and-text-together",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#layering-color-and-text-together",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "Sometimes want to use color to highlight some aspect of data\n\n\ncounty_data &lt;- socviz::county_data\n\n\n# DEM Blue and REP Red\nparty_colors &lt;- \n  c(\"#2E74C0\", \"#CB454A\") \n\np0 &lt;- ggplot(\n  data = filter(county_data, \n                flipped == \"No\"),\n  mapping = \n    aes(x = pop, \n        y = black/100) )\n\np1 &lt;- p0 + \n  geom_point(alpha = 0.15, \n             color = \"gray50\") \np1\n\n\n\n\n\n\n\n\n–&gt; Looks very skewed with the normal scale, so use a log10 scale instead\n\n p0 &lt;- ggplot(\n  data = filter(county_data, \n                flipped == \"No\"),\n  mapping = \n    aes(x = pop, \n        y = black/100) )\n\np1 &lt;- p0 + \n  geom_point(alpha = 0.15, \n             color = \"gray50\")+\n  scale_x_log10(labels=scales::comma)\n\np1\n\n\n\n\n\n\n\n\n–&gt; Now the data is more normally distributed, looks better as a viz, the last one was flipped == ‘No’, next will be flipped == ‘Yes’\n\np2 &lt;- p1 + \n  geom_point(\n    data = filter(county_data,\n                  flipped == \"Yes\"),\n    mapping = \n      aes(x = pop, y = black/100,\n          color = partywinner16)) +\n    scale_color_manual(\n      values = party_colors)\np2\n\n\n\n\n\n\n\n\n–&gt; viz will now be cleaned up and refined label-wise, and y-axis –&gt; % labels\n\np3 &lt;- p2 + \n  scale_y_continuous(\n    labels=scales::percent) +\n  labs(\n    color = \n      \"County flipped to ... \",\n    x = \n      \"County Population (log scale)\",\n    y = \n      \"Percent Black Population\",\n    title = \n      \"Flipped counties, 2016\",\n    caption = \n      \"Counties in gray did not flip.\")\np3\n\n\n\n\n\n\n\n\n–&gt; Next viz now labels state on points that were flipped and have % AA &gt; 25\n\nlibrary(ggrepel)\np4 &lt;- p3 + \n  geom_text_repel(\n    data = \n      filter(\n       county_data,\n       flipped == \"Yes\" & black &gt;25),\n    mapping = \n      aes(x = pop, y = black/100,\n          label = state), \n    size = 2)\n\np4 + theme_minimal() + \n  theme(legend.position=\"top\")"
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#changing-appearance-of-plots-with-theme",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#changing-appearance-of-plots-with-theme",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "p4 + theme_economist() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\np4 + theme_wsj() +\n  theme(\n    plot.title = \n      element_text(size = rel(0.6)), # rel() mean relative size, here sets title 60% of original size\n    legend.title = \n      element_text(size = rel(0.35)),\n    plot.caption = \n      element_text(size = rel(0.35)),\n    legend.position = \"top\")\n\n\n\n\n\n\n\n\n\np4 + theme(\n  legend.position = \"top\",\n  plot.title = element_text(\n      size = rel(2),\n      lineheight = .5,\n      family = \"Times\", # fonts not found, but this is how to set font type\n      face = \"bold.italic\", # how to set bolt/italic or both\n      color = \"orange\"),\n  axis.text.x = element_text(\n      size = rel(1.1),\n      family = \"Courier\",\n      face = \"bold\",\n      color = \"purple\")\n  )\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n–&gt; theme() allows us to have fine control over appearance of our visualizations\n\np4 + theme(\n  legend.position = \"top\",\n  plot.title = element_text(\n    size = rel(2),\n    lineheight = .5,\n    family = \"Times\",\n    face = \"bold.italic\",\n    colour = \"orange\"),\n  axis.text.x = element_blank()\n  ) \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n\n\n–&gt; element_blank() removes an element, as seen above with the x-axis labels\n\n\n\nShowing age distributiin of General Social Survey (GSS) respondents over the years\n\n\ngss_lon &lt;- socviz::gss_lon\n\n\n\n\nyrs &lt;- c(seq(1972, 1988, 4), \n         1993, \n         seq(1996, 2016, 4))\n\nmean_age &lt;- gss_lon |&gt;\n    filter( !is.na(age), \n            year %in% yrs) |&gt;\n    group_by(year) |&gt;\n    summarize(\n      xbar = round(\n        mean(age, na.rm = TRUE), 0)\n      ) # Removing obs in age var with missing vals, only for the years in yrs above, calc mean age for each year\n\nmean_age$y &lt;- 0.3 \n\nyr_labs &lt;- data.frame(\n  x = 85, y = 0.8, \n  year = yrs)  # to position the age as a text label\n\n\np &lt;- ggplot(\n  data = \n    filter(gss_lon, year %in% yrs),\n  mapping = \n    aes(x = age))\n\np1 &lt;- p + \n  geom_density(\n    fill = \"black\", color = FALSE,\n    alpha = 0.9, \n    mapping = aes(y = ..scaled..))\np1\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\n\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\np2 &lt;- p1 + \n  geom_vline(\n    data = filter(\n      mean_age, year %in% yrs),\n    aes(xintercept = xbar), \n    color = \"white\", linewidth = 0.5) + \n  geom_text(\n    data = filter(mean_age, \n             year %in% yrs),\n    aes(x = xbar, y = y, label = xbar), \n    nudge_x = 7.5, color = \"white\", \n    size = 3.5, hjust = 1) +\n  geom_text(data = filter(\n    yr_labs, year %in% yrs),\n    aes(x = x, y = y, label = year)) \np2\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n–&gt; nudge_x arg pushes label slightly to right of it’s associated x-value\n\np3 &lt;- p2  + \n  facet_grid(year ~ ., switch = \"y\")\np3\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n–&gt; switch = arg in facet_grid() moves labels to the left\n\np2a &lt;- p3 + \n  theme(\n    plot.title = \n      element_text(size = 16),\n    axis.text.x= \n      element_text(size = 12),\n    axis.title.y=element_blank(),\n    axis.text.y=element_blank(),\n    axis.ticks.y = element_blank(),\n    strip.background = element_blank(),\n    strip.text.y = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()) +\n  labs(x = \"Age\", y = NULL,\n       title = \n         \"Age Distribution of\\nGSS Respondents\")\np2a\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`)."
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#ggridges",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#ggridges",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "# install.packages(\"ggridges\")\nlibrary(ggridges)\n\n\nAllows distributions to overlap vertically\n\nespecially useful for repeated distributional measures that change in clear direction\n\n\n\np &lt;- ggplot(\n  data = gss_lon,\n  mapping = \n    aes(x = age, \n        y = factor(year, \n                   levels = rev(unique(year)), \n                   ordered = TRUE)))\n\n–&gt; factor() convets var to a factor var with set levels - levels param allows to set categories of categorical var\n\np &lt;- ggplot(\n  data = \n    filter(gss_lon, year %in% yrs),\n  mapping = \n    aes(x = age))\n\np2b &lt;- p + \n  geom_density_ridges(\n    alpha = 0.6, fill = \"lightblue\", \n    scale = 1.5,\n    mapping = aes(y = factor(year, levels = rev(unique(year)), ordered = TRUE))) +  \n    scale_x_continuous(\n      breaks = c(25, 50, 75)) +\n    scale_y_discrete(\n      expand = c(0.01, 0)) + \n    labs(x = \"Age\", y = NULL, \n         title = \n           \"Age Distribution of\\nGSS Respondents\") +\n    theme_ridges() +  # make labels aligned properly\n    theme(\n      title = \n        element_text(\n          size = 16, face = \"bold\"))\np2b\n\nPicking joint bandwidth of 3.48\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n–&gt; expand arg in scale_y_discrete() asjusts scalling of y-axis slightly"
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#arrange-plots-with-drigextragrid.arrange",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#arrange-plots-with-drigextragrid.arrange",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "library(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ngrid.arrange(p2a, p2b, nrow = 1)   # sub-figures\n\nDon't know how to automatically pick scale for object of type &lt;labelled&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nPicking joint bandwidth of 3.48\n\n\nWarning: Removed 83 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n–&gt; essentially takes multiple figures and arranges them in a table"
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#advanced-bar-charts",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#advanced-bar-charts",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "Using df studebt to show distribution of debt pct and how it varies by type\n\n\nstudebt &lt;- socviz::studebt\n\np_xlab &lt;- \"Amount Owed, in thousands of Dollars\"\np_title &lt;- \"Outstanding Student Loans\"\np_subtitle &lt;- \"44 million borrowers owe a total of $1.3 trillion\"\np_caption &lt;- \"Source: FRB NY\"\n\nf_labs &lt;- c(`Borrowers` = \"Percent of\\nall Borrowers\",\n            `Balances` = \"Percent of\\nall Balances\")\n\n\np &lt;- ggplot(\n  data = studebt,\n  mapping = \n    aes(x = pct/100, y = Debt,\n        fill = type))\np1 &lt;- p + geom_col()\np1\n\n\n\n\n\n\n\n\n\np2 &lt;- p1 +\n  scale_fill_brewer(\n    type = \"qual\", palette = \"Dark2\")\np2\n\n\n\n\n\n\n\n\n\np3 &lt;- p2 +\n  scale_x_continuous(\n    labels = scales::percent)\np3\n\n\n\n\n\n\n\n\n\np4 &lt;- p3 +\n  guides(fill = FALSE)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\np4\n\n\n\n\n\n\n\n\n–&gt; removing legend with guides(fill = FALSE)\n\np5 &lt;- p4 +\n  facet_grid(\n    .~ type, \n    labeller = as_labeller(f_labs))\np5\n\n\n\n\n\n\n\n\n–&gt; faceting here using custom labels as defined in f_labs at the beginning of this section\n\np6 &lt;- p5 +\n  labs(y = NULL, x = p_xlab, \n       caption = p_caption,\n       title = p_title,\n       subtitle = p_subtitle)\np6\n\n\n\n\n\n\n\n\n\np7 &lt;- p6 +\n  theme(strip.text.x = \n          element_text(face = \"bold\"))\np7"
  },
  {
    "objectID": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#advanced-bar-chart-2",
    "href": "danl_310_mats/danl_lec_refiningplots/danl_310_refiningplots.html#advanced-bar-chart-2",
    "title": "DANL 310 | Refining Plots",
    "section": "",
    "text": "Instead of having separate bars distinguished by height, - can use 100% stacked bars (proportions of bar)\nCan then facet and lay bars on sides for comparison\n\n\np &lt;- ggplot(\n  studebt, \n  aes(x = pct/100, y = type, \n      fill = Debtrc))\np1 &lt;- p + \n  geom_col(color = \"gray80\")+\n  theme(legend.position = 'top')\np1\n\n\n\n\n\n\n\n\n\np2 &lt;- p1 +\n  scale_y_discrete(\n    labels = as_labeller(f_labs))\np2\n\n\n\n\n\n\n\n\n\np3 &lt;- p2 +\n  scale_x_continuous(\n    labels = scales::percent)\np3\n\n\n\n\n\n\n\n\n\np4 &lt;- p3 +\n  scale_fill_viridis_d(\n    option = \"B\")\np4\n\n\n\n\n\n\n\n\n\np5 &lt;- p4 +\n  guides(\n    fill = \n      guide_legend(\n        reverse = TRUE,\n        title.position = \"top\",\n        label.position = \"bottom\",\n        keywidth = 3,\n        nrow = 1))\np5\n\n\n\n\n\n\n\n\n\np6 &lt;- p5 +\n  labs(x = NULL, y = NULL,\n       fill = \"Amount Owed, in thousands of dollars\",\n       caption = p_caption,\n       title = p_title,\n       subtitle = p_subtitle)\np6\n\n\n\n\n\n\n\n\n\np7 &lt;- p6 +\n  theme(legend.position = \"top\",\n        axis.text.y = \n          element_text(\n            face = \"bold\",\n            hjust = 1, \n            size = 12),\n        axis.ticks.length = \n          unit(0, \"cm\"),\n        panel.grid.major.y = \n          element_blank())\np7"
  },
  {
    "objectID": "danl_320.html",
    "href": "danl_320.html",
    "title": "DANL 320",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDANL 320 | Linear Regression w/ PySpark\n\n\n\n\n\n\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nBen & Jerry’s | Hw 2 Post\n\n\nBuilding a linear regression model with PySpark and an ice cream dataset\n\n\n\n\n\nMar 24, 2025\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nPySpark Basics | Hw 1 Post\n\n\nWithin this post the basics of PySpark will be discussed\n\n\n\n\n\nFeb 14, 2025\n\n\nDaniel Noone\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "",
    "text": "import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n# 1. Read CSV data from URL\ndf_pd = pd.read_csv('https://bcdanl.github.io/data/home_sales_nyc.csv')\nsale_df = spark.createDataFrame(df_pd)\nsale_df.show()\n\n+---------+-------+------------+--------------------+-----------------------+--------+----------+-----------------+----------------+-----------+----------------+-----------------+----------+---------+---+\n|sale.date|borough|borough_name|        neighborhood|building_class_category|zip_code|sale_price|residential_units|commercial_units|total_units|land_square_feet|gross_square_feet|year_built|sale_year|age|\n+---------+-------+------------+--------------------+-----------------------+--------+----------+-----------------+----------------+-----------+----------------+-----------------+----------+---------+---+\n|  8/31/18|      2|       Bronx|CASTLE HILL/UNION...|   01 ONE FAMILY DWE...|   10473|    492525|                1|               0|          1|            2250|             1540|      1960|     2018| 58|\n|  8/31/18|      2|       Bronx|           SOUNDVIEW|   01 ONE FAMILY DWE...|   10473|    105000|                1|               0|          1|            3980|             1025|      1910|     2018|108|\n|  8/31/18|      3|    Brooklyn|        BOROUGH PARK|   01 ONE FAMILY DWE...|   11204|   1765000|                1|               0|          1|            2805|             1864|      1930|     2018| 88|\n|  8/31/18|      3|    Brooklyn|          MILL BASIN|   01 ONE FAMILY DWE...|   11234|    950000|                1|               0|          1|            5200|             1852|      1960|     2018| 58|\n|  8/31/18|      4|      Queens|         FLORAL PARK|   01 ONE FAMILY DWE...|   11004|    566000|                1|               0|          1|            4000|              832|      1950|     2018| 68|\n|  8/31/18|      4|      Queens|      FLUSHING-NORTH|   01 ONE FAMILY DWE...|   11365|    839000|                1|               0|          1|            2600|             1438|      1950|     2018| 68|\n|  8/31/18|      4|      Queens|       FRESH MEADOWS|   01 ONE FAMILY DWE...|   11365|    985000|                1|               0|          1|            5900|             1668|      1945|     2018| 73|\n|  8/31/18|      4|      Queens|              HOLLIS|   01 ONE FAMILY DWE...|   11428|    499500|                1|               0|          1|            2400|             1152|      1925|     2018| 93|\n|  8/31/18|      4|      Queens|     JACKSON HEIGHTS|   01 ONE FAMILY DWE...|   11372|   1425000|                1|               0|          1|            3400|             2050|      1925|     2018| 93|\n|  8/31/18|      4|      Queens|           LAURELTON|   01 ONE FAMILY DWE...|   11413|    560000|                1|               0|          1|            3217|             1232|      1930|     2018| 88|\n|  8/31/18|      4|      Queens|      MIDDLE VILLAGE|   01 ONE FAMILY DWE...|   11378|    755000|                1|               0|          1|            1800|             1192|      1945|     2018| 73|\n|  8/31/18|      4|      Queens|          OZONE PARK|   01 ONE FAMILY DWE...|   11416|    499900|                1|               0|          1|            2500|             1224|      1915|     2018|103|\n|  8/31/18|      4|      Queens|          OZONE PARK|   01 ONE FAMILY DWE...|   11417|    560000|                1|               0|          1|            2800|             1272|      1925|     2018| 93|\n|  8/31/18|      4|      Queens| SPRINGFIELD GARDENS|   01 ONE FAMILY DWE...|   11434|    475000|                1|               0|          1|            3780|             1050|      1955|     2018| 63|\n|  8/31/18|      4|      Queens|          ST. ALBANS|   01 ONE FAMILY DWE...|   11412|    526875|                1|               0|          1|            5940|             1426|      1945|     2018| 73|\n|  8/31/18|      4|      Queens|          WHITESTONE|   01 ONE FAMILY DWE...|   11357|    773000|                1|               0|          1|            3770|             1056|      1950|     2018| 68|\n|  8/31/18|      4|      Queens|           WOODHAVEN|   01 ONE FAMILY DWE...|   11421|    665000|                1|               0|          1|            1710|             1364|      1920|     2018| 98|\n|  8/30/18|      2|       Bronx|           BRONXDALE|   01 ONE FAMILY DWE...|   10467|   1450000|                1|               1|          2|            6675|             2996|      1955|     2018| 63|\n|  8/30/18|      2|       Bronx|           RIVERDALE|   01 ONE FAMILY DWE...|   10463|    885000|                1|               0|          1|            5000|             2350|      1925|     2018| 93|\n|  8/30/18|      2|       Bronx|         THROGS NECK|   01 ONE FAMILY DWE...|   10465|    420000|                1|               0|          1|            3125|             2062|      1950|     2018| 68|\n+---------+-------+------------+--------------------+-----------------------+--------+----------+-----------------+----------------+-----------+----------------+-----------------+----------+---------+---+\nonly showing top 20 rows"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#splitting-data-into-train-and-test-sets",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#splitting-data-into-train-and-test-sets",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Splitting Data into Train and Test Sets",
    "text": "Splitting Data into Train and Test Sets\n\n# 2. Split data into training and testing sets by creating a random column \"gp\"\nsale_df = sale_df.withColumn(\"gp\", rand(seed=123)) # seed is set for replication\n\n# Splits 60-40 into training and test sets\ndtrain = sale_df.filter(col(\"gp\") &gt;= 0.4)\ndtest = sale_df.filter(col(\"gp\") &lt; 0.4)\n\n# Or simply,\ndtrain, dtest = sale_df.randomSplit([0.6, 0.4], seed = 123)"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#building-ml-df-using-vectorassembler",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#building-ml-df-using-vectorassembler",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Building ML DF Using VectorAssembler()",
    "text": "Building ML DF Using VectorAssembler()\n\nVectorAssembler\n\nTransformer in Pyspark ML Library\nUsed to combine multiple cols into single vector col\nMany ML Algs in spark req predictors to be represented as a single vector\nVectorAssembler often one of 1st steps in SparkML Pipeline\n\nVectorAssembler.transform() returns DF with new col, specified in outputCol arg in VectorAssembler\n\n\n# Now assemble predictors using the renamed column\nassembler1 = VectorAssembler(\n    inputCols=[\"gross_square_feet\"],\n    outputCol=\"predictors\")\n\ndtrain1 = assembler1.transform(dtrain) # training data\ndtest1  = assembler1.transform(dtest)  # test data\n\nprint('Training Set')\ndtrain1.select(\"predictors\", \"sale_price\").show()\n\nprint('Test Set')\ndtest1.select(\"predictors\", \"sale_price\").show()\n\nTraining Set\n+----------+----------+\n|predictors|sale_price|\n+----------+----------+\n|  [1742.0]|    349830|\n|  [2165.0]|    570000|\n|  [1116.0]|    410000|\n|  [1120.0]|    260000|\n|  [2304.0]|    543556|\n|  [1830.0]|    610000|\n|  [4200.0]|   1650000|\n|  [1446.0]|    905000|\n|  [1445.0]|   1150000|\n|   [978.0]|    850000|\n|  [1444.0]|    980000|\n|  [1510.0]|   1130000|\n|  [1428.0]|    425000|\n|  [1712.0]|    860000|\n|  [1200.0]|   1020000|\n|  [2363.0]|   1200000|\n|  [1197.0]|    427000|\n|  [1080.0]|    423000|\n|  [1392.0]|    325000|\n|  [1400.0]|    597000|\n+----------+----------+\nonly showing top 20 rows\n\nTest Set\n+----------+----------+\n|predictors|sale_price|\n+----------+----------+\n|  [1080.0]|    495000|\n|  [2320.0]|    575000|\n|  [2844.0]|   1700000|\n|  [1696.0]|    650000|\n|  [2000.0]|   1350000|\n|  [2300.0]|   1500000|\n|  [1314.0]|    940000|\n|  [2452.0]|    550000|\n|  [1125.0]|    820000|\n|  [1690.0]|    235000|\n|  [2163.0]|    695000|\n|  [1870.0]|    590000|\n|  [2052.0]|    715000|\n|  [1728.0]|    999000|\n|  [1650.0]|    720000|\n|  [1682.0]|   1150000|\n|  [1140.0]|    770000|\n|  [1782.0]|    795000|\n|  [1292.0]|    836000|\n|  [2487.0]|   1075000|\n+----------+----------+\nonly showing top 20 rows"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#building-lin-reg-model-using-linearregression.fit",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#building-lin-reg-model-using-linearregression.fit",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Building Lin Reg Model using LinearRegression().fit()",
    "text": "Building Lin Reg Model using LinearRegression().fit()\n\nLinearRegression(featuresCol = ‘predictors’, labelCol = ‘sale_price’)\n\nCreates instance of Linear Regression class\nfeatures are in a col named predictors\nlabel is in a column named sale_price\n\n.fit() trains lin reg model using training DF dtrain1\n\nthis process est beta coef. that best predict label from features\n\n\n\n# Fit linear regression model using the new label column \"sale_price\"\nmodel1 = (\n    LinearRegression(\n        featuresCol = \"predictors\",\n        labelCol = \"sale_price\")\n    .fit(dtrain1)\n)"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#summary-of-regression-result",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#summary-of-regression-result",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Summary of Regression Result",
    "text": "Summary of Regression Result\n\nitems= [model1.intercept,\nmodel1.coefficients,\nmodel1.summary.coefficientStandardErrors,\nmodel1.summary.rootMeanSquaredError,\nmodel1.summary.r2]\n\nfor i in items:\n  print(i)\n\n-28248.33125480523\n[456.5835962723607]\n[9.1585344019424, 14842.026122561703]\n459852.1900902878\n0.24763531485957802\n\n\n\nMaking Summary Look Better\n\nimport numpy as np\nimport scipy.stats as stats\nfrom tabulate import tabulate\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler,\n    and inserts a dashed horizontal line after the Intercept row. The table includes separate columns\n    for the 95% confidence interval lower and upper bounds for each coefficient (computed at the 5% significance level)\n    and an \"Observations\" row (using model.summary.numInstances) above the R² row.\n    The RMSE row is placed as the last row.\n\n    The columns are ordered as:\n        Metric | Value | Significance | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    For the \"Value\", \"Std. Error\", \"95% CI Lower\", and \"95% CI Upper\" columns, commas are inserted every three digits,\n    with 3 decimal places (except for Observations which is formatted as an integer with commas).\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Extract coefficients and standard errors as NumPy arrays\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element)\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Compute t-statistics for feature coefficients (t = beta / SE(beta))\n    # t_stats = coeffs / std_errors\n    t_stats = model.summary.tValues\n\n    # Degrees of freedom: number of instances minus number of predictors minus 1 (for intercept)\n    df = model.summary.numInstances - len(coeffs) - 1\n\n    # Compute the t-critical value for a 95% confidence interval (two-tailed, 5% significance)\n    t_critical = stats.t.ppf(0.975, df)\n\n    # Compute two-tailed p-values for each feature coefficient\n    # p_values = [2 * (1 - stats.t.cdf(np.abs(t), df)) for t in t_stats]\n    p_values = model.summary.pValues\n\n    # Function to assign significance stars based on p-value\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build the table rows.\n    # Order: Metric, Value, Significance, Std. Error, p-value, 95% CI Lower, 95% CI Upper.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n        table.append([\n            \"Beta: \" + feature,       # Metric name\n            beta,                     # Beta estimate (Value)\n            significance_stars(p),    # Significance stars\n            se,                       # Standard error\n            p,                        # p-value\n            ci_lower,                 # 95% CI lower bound\n            ci_upper                  # 95% CI upper bound\n        ])\n\n    # Compute and add the intercept row with its SE, p-value, significance, and CI (if available)\n    if intercept_se is not None:\n        intercept_t = model.intercept / intercept_se\n        intercept_p = 2 * (1 - stats.t.cdf(np.abs(intercept_t), df))\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_se = \"\"\n        intercept_p = \"\"\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n\n    table.append([\n        \"Intercept\",\n        model.intercept,\n        intercept_sig,\n        intercept_se,\n        intercept_p,\n        ci_intercept_lower,\n        ci_intercept_upper\n    ])\n\n    # Append overall model metrics:\n    # Insert an Observations row using model.summary.numInstances,\n    # then an R² row, and finally the RMSE row as the last row.\n    table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table.\n    # For the \"Value\" (index 1), \"Std. Error\" (index 3), \"95% CI Lower\" (index 5), and \"95% CI Upper\" (index 6) columns,\n    # format with commas and 3 decimal places, except for Observations which should be an integer with commas.\n    # For the p-value (index 4), format to 3 decimal places.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                # Format Observations as integer with commas, no decimals.\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if i in [1, 3, 5, 6]:\n                    formatted_row.append(f\"{item:,.3f}\")\n                elif i == 4:\n                    formatted_row.append(f\"{item:.3f}\")\n                else:\n                    formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Generate the table string using tabulate.\n    table_str = tabulate(\n        formatted_table,\n        headers=[\"Metric\", \"Value\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"],\n        tablefmt=\"pretty\",\n        colalign=(\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n    )\n\n    # Insert a dashed line after the Intercept row for clarity.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(lr_model, assembler))\n\n\nprint(regression_table(model1, assembler1))\n\n+-------------------------+-------------+------+------------+---------+--------------+--------------+\n| Metric                  |       Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+-------------------------+-------------+------+------------+---------+--------------+--------------+\n| Beta: gross_square_feet |     456.584 | ***  | 14,842.026 |   0.000 |  -28,637.917 |   29,551.084 |\n| Intercept               | -28,248.331 | ***  |      9.159 |   0.000 |  -28,266.285 |  -28,230.378 |\n-----------------------------------------------------------------------------------------------------\n| Observations            |       7,553 |      |            |         |              |              |\n| R²                      |       0.248 |      |            |         |              |              |\n| RMSE                    | 459,852.190 |      |            |         |              |              |\n+-------------------------+-------------+------+------------+---------+--------------+--------------+"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#adding-new-predictor-age-to-vectorassembler",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#adding-new-predictor-age-to-vectorassembler",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Adding new predictor age to VectorAssembler()",
    "text": "Adding new predictor age to VectorAssembler()\n\nassembler2 = VectorAssembler(\n                    inputCols=[\"gross_square_feet\", \"age\"],\n                    outputCol=\"predictors\")\ndtrain2 = assembler2.transform(dtrain)\ndtest2  = assembler2.transform(dtest)\n\nmodel2 = LinearRegression(\n                featuresCol=\"predictors\",\n                labelCol=\"sale_price\").fit(dtrain2)\ndtest2 = model2.transform(dtest2)\n\nprint(regression_table(model2, assembler2))\n\n+-------------------------+--------------+------+------------+---------+--------------+--------------+\n| Metric                  |        Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+-------------------------+--------------+------+------------+---------+--------------+--------------+\n| Beta: gross_square_feet |      463.107 | ***  |    192.136 |   0.000 |       86.467 |      839.748 |\n| Beta: age               |    2,429.358 | ***  | 20,938.514 |   0.000 |  -38,615.955 |   43,474.670 |\n| Intercept               | -216,925.023 | ***  |      9.078 |   0.000 | -216,942.819 | -216,907.227 |\n------------------------------------------------------------------------------------------------------\n| Observations            |        7,553 |      |            |         |              |              |\n| R²                      |        0.263 |      |            |         |              |              |\n| RMSE                    |  455,059.540 |      |            |         |              |              |\n+-------------------------+--------------+------+------------+---------+--------------+--------------+"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#residual-plots",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#residual-plots",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nScatterplot of fitted values and residuals\n\nfitted values on x-axis and residuals on y-axis\n\nCan be used to diagnose quality of model results\nAssumed that epsilon i (random noise, residual error estimates it), has mean of 0 with constant variance\n\nwell-behaved residual plot should bounce randomly and form a cloud roughly at zero residual level (around the x-axis, where x = 0 represents perfect prediction)\n\nesentially asking two questions… \n\n\nOn average are predictions correct (is there a cloud around 0-residual line?)\nAre there systematic errors (is there seeming to be a constant variance?, does it fan out? Stay close along 0-residual line?)"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#unbiasedness-and-homoskedasticity",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#unbiasedness-and-homoskedasticity",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Unbiasedness and Homoskedasticity",
    "text": "Unbiasedness and Homoskedasticity\n\nWould like residual plot to be:\nUnbiased:\n\nHave an average value of 0 in any thin vertical stretch\n\nHomoskedastic:\n\nHave same spread of residuals in any thin vertical strip (ie; not fanning out, so to say)\n\nWhen the variance of residuals changes (not remaining constant) across predicted values, model suffers from heteroskedasticity\n\n\n\nIf model is biased:\nConsistently overpredicts or underpredicts for certain values\n\nIndicates model may be misspecified (missing importants vars or using incorrect functional form)\nLeads to biased param estimates, meaning coef are off, reducing validity of predictions and inferences\n\nIf model is heteroscedastic:\n\nInefficient coef estimates\n\nestimates remain unbiased but are no longer efficient (e.g. don’t have smallest variance)\n\nBiased standard errors\n\nleads to unreliable p-vals and CIs, maybe resulting in invaling hyp-tests\n\nMisleading inferences\n\nPredictors maybe misrepresented as statistically significant or insignificant\n\nPoor predictive performance\n\nmodel might perform poorly on future data (poor generalization), especially if residual variance grows with higher predicted values"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#residual-plots-in-python",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#residual-plots-in-python",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Residual Plots in Python",
    "text": "Residual Plots in Python\nPySpark does NOT have built-in viz capabilities - Have to convert PySpark DF to Pandas DF by using .toPandas()\n\n# Residual plot for Model 2:\n# Convert test predictions to Pandas\nrdf = dtest2.select([\"prediction\", \"sale_price\"]).toPandas()\n# Pred and actual following fitting data to model, on test set specifically\nrdf[\"residual\"] = rdf[\"sale_price\"] - rdf[\"prediction\"]\n\nrdf\n\n\n  \n    \n\n\n\n\n\n\nprediction\nsale_price\nresidual\n\n\n\n\n0\n5.577485e+05\n495000\n-62748.475868\n\n\n1\n1.095561e+06\n575000\n-520561.348096\n\n\n2\n1.384387e+06\n1700000\n315612.545346\n\n\n3\n7.822887e+05\n650000\n-132288.717881\n\n\n4\n9.473670e+05\n1350000\n402633.036838\n\n\n...\n...\n...\n...\n\n\n5110\n5.196712e+05\n467375\n-52296.218118\n\n\n5111\n1.519552e+06\n1610000\n90448.492334\n\n\n5112\n6.282781e+05\n365000\n-263278.103427\n\n\n5113\n3.512530e+05\n235000\n-116253.049319\n\n\n5114\n3.713118e+05\n480000\n108688.210815\n\n\n\n\n5115 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nThen use matplotlib.pyplot to do a residual plot\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm  # for lowess smoothing\n\nplt.scatter(rdf[\"prediction\"], rdf[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(rdf[\"residual\"], rdf[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n# Perfect prediction line\n#   A horizontal line at residual = 0\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n# Labeling\nplt.xlabel(\"Predicted sale.price (Model 2)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model 2\")\nplt.show()\n\nprint(regression_table(model2, assembler2))\n\n\n\n\n\n\n\n\n+-------------------------+--------------+------+------------+---------+--------------+--------------+\n| Metric                  |        Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+-------------------------+--------------+------+------------+---------+--------------+--------------+\n| Beta: gross_square_feet |      463.107 | ***  |    192.136 |   0.000 |       86.467 |      839.748 |\n| Beta: age               |    2,429.358 | ***  | 20,938.514 |   0.000 |  -38,615.955 |   43,474.670 |\n| Intercept               | -216,925.023 | ***  |      9.078 |   0.000 | -216,942.819 | -216,907.227 |\n------------------------------------------------------------------------------------------------------\n| Observations            |        7,553 |      |            |         |              |              |\n| R²                      |        0.263 |      |            |         |              |              |\n| RMSE                    |  455,059.540 |      |            |         |              |              |\n+-------------------------+--------------+------+------------+---------+--------------+--------------+"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#log-transformed-vars-in-pyspark",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#log-transformed-vars-in-pyspark",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Log-transformed Vars in PySpark",
    "text": "Log-transformed Vars in PySpark\n\nfrom pyspark.sql.functions import log\nimport numpy\nsale_df = sale_df.withColumn(\"log_sale_price\",\n                              log( sale_df['sale_price'] ) )\nsale_df.show()\n\n+---------+-------+------------+--------------------+-----------------------+--------+----------+-----------------+----------------+-----------+----------------+-----------------+----------+---------+---+------------------+\n|sale.date|borough|borough_name|        neighborhood|building_class_category|zip_code|sale_price|residential_units|commercial_units|total_units|land_square_feet|gross_square_feet|year_built|sale_year|age|    log_sale_price|\n+---------+-------+------------+--------------------+-----------------------+--------+----------+-----------------+----------------+-----------+----------------+-----------------+----------+---------+---+------------------+\n|  8/31/18|      2|       Bronx|CASTLE HILL/UNION...|   01 ONE FAMILY DWE...|   10473|    492525|                1|               0|          1|            2250|             1540|      1960|     2018| 58|13.107300499727284|\n|  8/31/18|      2|       Bronx|           SOUNDVIEW|   01 ONE FAMILY DWE...|   10473|    105000|                1|               0|          1|            3980|             1025|      1910|     2018|108| 11.56171562913966|\n|  8/31/18|      3|    Brooklyn|        BOROUGH PARK|   01 ONE FAMILY DWE...|   11204|   1765000|                1|               0|          1|            2805|             1864|      1930|     2018| 88|14.383661248349535|\n|  8/31/18|      3|    Brooklyn|          MILL BASIN|   01 ONE FAMILY DWE...|   11234|    950000|                1|               0|          1|            5200|             1852|      1960|     2018| 58|13.764217263576723|\n|  8/31/18|      4|      Queens|         FLORAL PARK|   01 ONE FAMILY DWE...|   11004|    566000|                1|               0|          1|            4000|              832|      1950|     2018| 68| 13.24634935718532|\n|  8/31/18|      4|      Queens|      FLUSHING-NORTH|   01 ONE FAMILY DWE...|   11365|    839000|                1|               0|          1|            2600|             1438|      1950|     2018| 68|13.639965985449344|\n|  8/31/18|      4|      Queens|       FRESH MEADOWS|   01 ONE FAMILY DWE...|   11365|    985000|                1|               0|          1|            5900|             1668|      1945|     2018| 73|13.800396920154226|\n|  8/31/18|      4|      Queens|              HOLLIS|   01 ONE FAMILY DWE...|   11428|    499500|                1|               0|          1|            2400|             1152|      1925|     2018| 93|13.121362877070744|\n|  8/31/18|      4|      Queens|     JACKSON HEIGHTS|   01 ONE FAMILY DWE...|   11372|   1425000|                1|               0|          1|            3400|             2050|      1925|     2018| 93|14.169682371684887|\n|  8/31/18|      4|      Queens|           LAURELTON|   01 ONE FAMILY DWE...|   11413|    560000|                1|               0|          1|            3217|             1232|      1930|     2018| 88|13.235692062711331|\n|  8/31/18|      4|      Queens|      MIDDLE VILLAGE|   01 ONE FAMILY DWE...|   11378|    755000|                1|               0|          1|            1800|             1192|      1945|     2018| 73|13.534473028231162|\n|  8/31/18|      4|      Queens|          OZONE PARK|   01 ONE FAMILY DWE...|   11416|    499900|                1|               0|          1|            2500|             1224|      1915|     2018|103|13.122163357401663|\n|  8/31/18|      4|      Queens|          OZONE PARK|   01 ONE FAMILY DWE...|   11417|    560000|                1|               0|          1|            2800|             1272|      1925|     2018| 93|13.235692062711331|\n|  8/31/18|      4|      Queens| SPRINGFIELD GARDENS|   01 ONE FAMILY DWE...|   11434|    475000|                1|               0|          1|            3780|             1050|      1955|     2018| 63|13.071070083016778|\n|  8/31/18|      4|      Queens|          ST. ALBANS|   01 ONE FAMILY DWE...|   11412|    526875|                1|               0|          1|            5940|             1426|      1945|     2018| 73|13.174718607738257|\n|  8/31/18|      4|      Queens|          WHITESTONE|   01 ONE FAMILY DWE...|   11357|    773000|                1|               0|          1|            3770|             1056|      1950|     2018| 68|13.558034327569558|\n|  8/31/18|      4|      Queens|           WOODHAVEN|   01 ONE FAMILY DWE...|   11421|    665000|                1|               0|          1|            1710|             1364|      1920|     2018| 98|13.407542319637992|\n|  8/30/18|      2|       Bronx|           BRONXDALE|   01 ONE FAMILY DWE...|   10467|   1450000|                1|               1|          2|            6675|             2996|      1955|     2018| 63|14.187074114396758|\n|  8/30/18|      2|       Bronx|           RIVERDALE|   01 ONE FAMILY DWE...|   10463|    885000|                1|               0|          1|            5000|             2350|      1925|     2018| 93|13.693342923990066|\n|  8/30/18|      2|       Bronx|         THROGS NECK|   01 ONE FAMILY DWE...|   10465|    420000|                1|               0|          1|            3125|             2062|      1950|     2018| 68|12.948009990259552|\n+---------+-------+------------+--------------------+-----------------------+--------+----------+-----------------+----------------+-----------+----------------+-----------------+----------+---------+---+------------------+\nonly showing top 20 rows"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q1",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q1",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Q1",
    "text": "Q1\n\nbikeshare = pd.read_csv('https://bcdanl.github.io/data/bikeshare_cleaned.csv')\nbikeshare = spark.createDataFrame(bikeshare)\nbikeshare.show()\n\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\n|cnt|year|month|date| hr|   wkday|holiday|seasons|        weather_cond|              temp|              hum|         windspeed|\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\n| 16|2011|    1|   1|  0|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.947345243330896|  -1.5538438052971|\n| 40|2011|    1|   1|  1|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n| 32|2011|    1|   1|  2|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n| 13|2011|    1|   1|  3|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n|  1|2011|    1|   1|  4|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n|  1|2011|    1|   1|  5|saturday|      0| spring|      Mist or Cloudy| -1.33460918694128|0.636351351217591|-0.821460017517193|\n|  2|2011|    1|   1|  6|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n|  3|2011|    1|   1|  7|saturday|      0| spring| Clear or Few Cloudy| -1.54234083286556| 1.20650682009198|  -1.5538438052971|\n|  8|2011|    1|   1|  8|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n| 14|2011|    1|   1|  9|saturday|      0| spring| Clear or Few Cloudy|-0.919145895092722|0.688183666569809|  -1.5538438052971|\n| 36|2011|    1|   1| 10|saturday|      0| spring| Clear or Few Cloudy|-0.607548426206302|0.688183666569809| 0.519881272378821|\n| 56|2011|    1|   1| 11|saturday|      0| spring| Clear or Few Cloudy|-0.711414249168442|0.947345243330896| 0.764281665845554|\n| 84|2011|    1|   1| 12|saturday|      0| spring| Clear or Few Cloudy|-0.399816780282022|0.740015981922026| 0.764281665845554|\n| 94|2011|    1|   1| 13|saturday|      0| spring|      Mist or Cloudy|-0.192085134357741|0.480854405160939| 0.886073166268775|\n|106|2011|    1|   1| 14|saturday|      0| spring|      Mist or Cloudy|-0.192085134357741|0.480854405160939| 0.764281665845554|\n|110|2011|    1|   1| 15|saturday|      0| spring|      Mist or Cloudy|-0.295950957319881|0.740015981922026| 0.886073166268775|\n| 93|2011|    1|   1| 16|saturday|      0| spring|      Mist or Cloudy|-0.399816780282022|0.999177558683113| 0.886073166268775|\n| 67|2011|    1|   1| 17|saturday|      0| spring|      Mist or Cloudy|-0.295950957319881|0.999177558683113| 0.764281665845554|\n| 35|2011|    1|   1| 18|saturday|      0| spring|Light Snow or Lig...|-0.399816780282022| 1.31017145079642| 0.519881272378821|\n| 37|2011|    1|   1| 19|saturday|      0| spring|Light Snow or Lig...|-0.399816780282022| 1.31017145079642| 0.519881272378821|\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\nonly showing top 20 rows\n\n\n\n\nbikeshare.describe().show()\n\n+-------+-----------------+------------------+------------------+------------------+------------------+---------+--------------------+-------+-------------------+--------------------+--------------------+--------------------+\n|summary|              cnt|              year|             month|              date|                hr|    wkday|             holiday|seasons|       weather_cond|                temp|                 hum|           windspeed|\n+-------+-----------------+------------------+------------------+------------------+------------------+---------+--------------------+-------+-------------------+--------------------+--------------------+--------------------+\n|  count|            17376|             17376|             17376|             17376|             17376|    17376|               17376|  17376|              17376|               17376|               17376|               17376|\n|   mean|189.4829650092081| 2011.502532228361| 6.538731583793738|15.682895948434622|11.546731123388582|     NULL|0.028775322283609576|   NULL|               NULL|2.782432173414443E-4|-2.29186617616944...|-1.96196719787483...|\n| stddev|181.3950218567111|0.5000079758750275|3.4383025228315445| 8.789544988430704| 6.914283222380914|     NULL|  0.1671792797835931|   NULL|               NULL|  0.9998593726137501|  0.9999316512968688|  1.0000301062750698|\n|    min|                1|              2011|                 1|                 1|                 0|   friday|                   0|   fall|Clear or Few Cloudy|   -2.47713323952482|   -3.25107230019872|    -1.5538438052971|\n|    max|              977|              2012|                12|                31|                23|wednesday|                   1| winter|     Mist or Cloudy|    2.61229208562004|    1.93215923502303|    5.39971521551057|\n+-------+-----------------+------------------+------------------+------------------+------------------+---------+--------------------+-------+-------------------+--------------------+--------------------+--------------------+\n\n\n\n\n(\n    bikeshare\n    .groupby('holiday')\n    .count()\n    .show() # Can be done for each var\n)\n\n+-------+-----+\n|holiday|count|\n+-------+-----+\n|      0|16876|\n|      1|  500|\n+-------+-----+"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q2---splitting-into-test-and-train-sets",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q2---splitting-into-test-and-train-sets",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Q2 - Splitting into test and train sets",
    "text": "Q2 - Splitting into test and train sets\n\nbikeshare = bikeshare.withColumn(\"gp\", rand(seed=123)) # seed is set for replication\n\n# Splits 60-40 into training and test sets\ndtrain = bikeshare.filter(col(\"gp\") &gt;= 0.4)\ndtest = bikeshare.filter(col(\"gp\") &lt; 0.4)\n\n# Or simply,\ndtrain, dtest = bikeshare.randomSplit([0.6, 0.4], seed = 123)"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q3---training-model-and-splitting-cat-dummy",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q3---training-model-and-splitting-cat-dummy",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Q3 - Training model and splitting cat –> dummy",
    "text": "Q3 - Training model and splitting cat –&gt; dummy\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\nimport numpy as np\nimport scipy.stats as stats\nfrom tabulate import tabulate\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler,\n    and inserts a dashed horizontal line after the Intercept row. The table includes separate columns\n    for the 95% confidence interval lower and upper bounds for each coefficient (computed at the 5% significance level)\n    and an \"Observations\" row (using model.summary.numInstances) above the R² row.\n    The RMSE row is placed as the last row.\n\n    The columns are ordered as:\n        Metric | Value | Significance | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    For the \"Value\", \"Std. Error\", \"95% CI Lower\", and \"95% CI Upper\" columns, commas are inserted every three digits,\n    with 3 decimal places (except for Observations which is formatted as an integer with commas).\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Extract coefficients and standard errors as NumPy arrays\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element)\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Compute t-statistics for feature coefficients (t = beta / SE(beta))\n    # t_stats = coeffs / std_errors\n    t_stats = model.summary.tValues\n\n    # Degrees of freedom: number of instances minus number of predictors minus 1 (for intercept)\n    df = model.summary.numInstances - len(coeffs) - 1\n\n    # Compute the t-critical value for a 95% confidence interval (two-tailed, 5% significance)\n    t_critical = stats.t.ppf(0.975, df)\n\n    # Compute two-tailed p-values for each feature coefficient\n    # p_values = [2 * (1 - stats.t.cdf(np.abs(t), df)) for t in t_stats]\n    p_values = model.summary.pValues\n\n    # Function to assign significance stars based on p-value\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build the table rows.\n    # Order: Metric, Value, Significance, Std. Error, p-value, 95% CI Lower, 95% CI Upper.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n        table.append([\n            \"Beta: \" + feature,       # Metric name\n            beta,                     # Beta estimate (Value)\n            significance_stars(p),    # Significance stars\n            se,                       # Standard error\n            p,                        # p-value\n            ci_lower,                 # 95% CI lower bound\n            ci_upper                  # 95% CI upper bound\n        ])\n\n    # Compute and add the intercept row with its SE, p-value, significance, and CI (if available)\n    if intercept_se is not None:\n        intercept_t = model.intercept / intercept_se\n        intercept_p = 2 * (1 - stats.t.cdf(np.abs(intercept_t), df))\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_se = \"\"\n        intercept_p = \"\"\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n\n    table.append([\n        \"Intercept\",\n        model.intercept,\n        intercept_sig,\n        intercept_se,\n        intercept_p,\n        ci_intercept_lower,\n        ci_intercept_upper\n    ])\n\n    # Append overall model metrics:\n    # Insert an Observations row using model.summary.numInstances,\n    # then an R² row, and finally the RMSE row as the last row.\n    table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table.\n    # For the \"Value\" (index 1), \"Std. Error\" (index 3), \"95% CI Lower\" (index 5), and \"95% CI Upper\" (index 6) columns,\n    # format with commas and 3 decimal places, except for Observations which should be an integer with commas.\n    # For the p-value (index 4), format to 3 decimal places.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                # Format Observations as integer with commas, no decimals.\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if i in [1, 3, 5, 6]:\n                    formatted_row.append(f\"{item:,.3f}\")\n                elif i == 4:\n                    formatted_row.append(f\"{item:.3f}\")\n                else:\n                    formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Generate the table string using tabulate.\n    table_str = tabulate(\n        formatted_table,\n        headers=[\"Metric\", \"Value\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"],\n        tablefmt=\"pretty\",\n        colalign=(\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n    )\n\n    # Insert a dashed line after the Intercept row for clarity.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(lr_model, assembler))\n\n\ndummy_cols_year, ref_cat_year = add_dummy_variables('year', 0)\ndummy_cols_month, ref_cat_month = add_dummy_variables('month', 0)\ndummy_cols_hr, ref_cat_hr = add_dummy_variables('hr', 0)\ndummy_cols_hol, ref_cat_hol = add_dummy_variables('holiday', 0)\ndummy_cols_wc, ref_cat_wc = add_dummy_variables('weather_cond', 0)\n\ncustom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\ndummy_cols_day, ref_cat_day = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\ncustom_order_season = ['spring', 'summer', 'fall', 'winter']\ndummy_cols_season, ref_cat_season = add_dummy_variables('seasons', reference_level=0, category_order = custom_order_season)\n\ncont_cols = ['temp', 'hum', 'windspeed']\n\nfeatures = cont_cols + dummy_cols_year + dummy_cols_month + dummy_cols_hr + dummy_cols_hol + dummy_cols_wc + dummy_cols_day + dummy_cols_season\n\nReference category (dummy omitted): 2011\nReference category (dummy omitted): 1\nReference category (dummy omitted): 0\nReference category (dummy omitted): 0\nReference category (dummy omitted): Clear or Few Cloudy\nReference category (dummy omitted): sunday\nReference category (dummy omitted): spring\n\n\n\nassembler_d = VectorAssembler(\n                    inputCols= features,\n                    outputCol=\"predictors\")\ndtrain2 = assembler_d.transform(dtrain)\ndtest2  = assembler_d.transform(dtest)\n\nmodel_d = LinearRegression(\n                featuresCol=\"predictors\",\n                labelCol=\"cnt\").fit(dtrain2)\ndtest2 = model_d.transform(dtest2)\n\n\nprint(regression_table(model_d, assembler_d))\n\n+---------------------------------------------+---------+------+------------+---------+--------------+--------------+\n| Metric                                      |   Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+---------------------------------------------+---------+------+------------+---------+--------------+--------------+\n| Beta: temp                                  |  42.329 | ***  |      1.396 |   0.000 |       39.592 |       45.066 |\n| Beta: hum                                   | -15.818 | ***  |      1.095 |   0.000 |      -17.964 |      -13.673 |\n| Beta: windspeed                             |  -4.592 | ***  |      2.033 |   0.000 |       -8.577 |       -0.607 |\n| Beta: year_2012                             |  85.733 | ***  |      5.067 |   0.000 |       75.801 |       95.664 |\n| Beta: month_2                               |   5.204 |      |      5.702 |   0.304 |       -5.973 |       16.380 |\n| Beta: month_3                               |  13.331 |  **  |      8.473 |   0.019 |       -3.278 |       29.940 |\n| Beta: month_4                               |   6.755 |      |      9.060 |   0.425 |      -11.004 |       24.515 |\n| Beta: month_5                               |  17.228 |  *   |      9.321 |   0.057 |       -1.042 |       35.499 |\n| Beta: month_6                               |   8.092 |      |     10.481 |   0.385 |      -12.453 |       28.638 |\n| Beta: month_7                               |  -9.133 |      |     10.181 |   0.384 |      -29.090 |       10.825 |\n| Beta: month_8                               |  15.455 |      |      9.060 |   0.129 |       -2.304 |       33.215 |\n| Beta: month_9                               |  34.747 | ***  |      8.383 |   0.000 |       18.314 |       51.180 |\n| Beta: month_10                              |  13.061 |      |      8.089 |   0.119 |       -2.795 |       28.918 |\n| Beta: month_11                              | -10.117 |      |      6.398 |   0.211 |      -22.658 |        2.424 |\n| Beta: month_12                              |  -6.010 |      |      6.856 |   0.348 |      -19.448 |        7.429 |\n| Beta: hr_1                                  | -18.319 | ***  |      6.844 |   0.008 |      -31.735 |       -4.902 |\n| Beta: hr_2                                  | -26.524 | ***  |      7.037 |   0.000 |      -40.319 |      -12.730 |\n| Beta: hr_3                                  | -40.579 | ***  |      6.946 |   0.000 |      -54.195 |      -26.964 |\n| Beta: hr_4                                  | -35.211 | ***  |      6.830 |   0.000 |      -48.600 |      -21.823 |\n| Beta: hr_5                                  | -21.258 | ***  |      6.896 |   0.002 |      -34.775 |       -7.740 |\n| Beta: hr_6                                  |  33.752 | ***  |      6.850 |   0.000 |       20.324 |       47.181 |\n| Beta: hr_7                                  | 175.927 | ***  |      6.821 |   0.000 |      162.556 |      189.298 |\n| Beta: hr_8                                  | 315.633 | ***  |      6.908 |   0.000 |      302.092 |      329.174 |\n| Beta: hr_9                                  | 164.763 | ***  |      6.850 |   0.000 |      151.337 |      178.190 |\n| Beta: hr_10                                 | 113.238 | ***  |      6.906 |   0.000 |       99.701 |      126.776 |\n| Beta: hr_11                                 | 134.600 | ***  |      6.910 |   0.000 |      121.055 |      148.145 |\n| Beta: hr_12                                 | 173.827 | ***  |      7.187 |   0.000 |      159.739 |      187.916 |\n| Beta: hr_13                                 | 166.014 | ***  |      7.172 |   0.000 |      151.955 |      180.074 |\n| Beta: hr_14                                 | 149.687 | ***  |      7.102 |   0.000 |      135.766 |      163.608 |\n| Beta: hr_15                                 | 166.505 | ***  |      7.093 |   0.000 |      152.601 |      180.408 |\n| Beta: hr_16                                 | 231.115 | ***  |      7.034 |   0.000 |      217.326 |      244.904 |\n| Beta: hr_17                                 | 373.796 | ***  |      6.927 |   0.000 |      360.219 |      387.374 |\n| Beta: hr_18                                 | 345.005 | ***  |      6.900 |   0.000 |      331.480 |      358.531 |\n| Beta: hr_19                                 | 236.633 | ***  |      6.948 |   0.000 |      223.013 |      250.252 |\n| Beta: hr_20                                 | 156.109 | ***  |      6.801 |   0.000 |      142.777 |      169.441 |\n| Beta: hr_21                                 | 109.951 | ***  |      6.870 |   0.000 |       96.485 |      123.417 |\n| Beta: hr_22                                 |  72.172 | ***  |      6.827 |   0.000 |       58.790 |       85.554 |\n| Beta: hr_23                                 |  35.565 | ***  |      6.242 |   0.000 |       23.329 |       47.801 |\n| Beta: holiday_1                             | -28.657 | ***  |      4.148 |   0.000 |      -36.787 |      -20.527 |\n| Beta: weather_cond_Light_Snow_or_Light_Rain | -67.007 | ***  |      2.491 |   0.000 |      -71.889 |      -62.125 |\n| Beta: weather_cond_Mist_or_Cloudy           |  -9.108 | ***  |      3.888 |   0.000 |      -16.729 |       -1.486 |\n| Beta: wkday_monday                          |  12.929 | ***  |      3.787 |   0.001 |        5.506 |       20.352 |\n| Beta: wkday_tuesday                         |  14.432 | ***  |      3.769 |   0.000 |        7.044 |       21.821 |\n| Beta: wkday_wednesday                       |  16.434 | ***  |      3.788 |   0.000 |        9.010 |       23.859 |\n| Beta: wkday_thursday                        |  16.409 | ***  |      3.751 |   0.000 |        9.056 |       23.762 |\n| Beta: wkday_friday                          |  18.646 | ***  |      3.766 |   0.000 |       11.264 |       26.029 |\n| Beta: wkday_saturday                        |  19.887 | ***  |      6.231 |   0.000 |        7.673 |       32.100 |\n| Beta: seasons_summer                        |  42.796 | ***  |      7.429 |   0.000 |       28.233 |       57.358 |\n| Beta: seasons_fall                          |  29.693 | ***  |      6.289 |   0.000 |       17.366 |       42.021 |\n| Beta: seasons_winter                        |  68.686 | ***  |      7.452 |   0.000 |       54.078 |       83.294 |\n| Intercept                                   | -27.648 | ***  |      2.362 |   0.000 |      -32.278 |      -23.017 |\n---------------------------------------------------------------------------------------------------------------------\n| Observations                                |  10,375 |      |            |         |              |              |\n| R²                                          |   0.686 |      |            |         |              |              |\n| RMSE                                        | 101.792 |      |            |         |              |              |\n+---------------------------------------------+---------+------+------------+---------+--------------+--------------+"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q7",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q7",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Q7",
    "text": "Q7\n\nfeat = assembler_d.getInputCols()\ncf = model_d.coefficients.toArray()[:len(feat)]\nerr = model_d.summary.coefficientStandardErrors[:len(feat)]\n\ndf_cw8 = pd.DataFrame({\n    'features': feat,\n    'est': cf,\n    'std_error': err\n})\n\ndf_cw8\n\n\n  \n    \n\n\n\n\n\n\nfeatures\nest\nstd_error\n\n\n\n\n0\ntemp\n42.328840\n2.362286\n\n\n1\nhum\n-15.818284\n1.396342\n\n\n2\nwindspeed\n-4.591617\n1.094665\n\n\n3\nyear_2012\n85.732784\n2.032948\n\n\n4\nmonth_2\n5.203765\n5.066519\n\n\n5\nmonth_3\n13.331414\n5.701653\n\n\n6\nmonth_4\n6.755455\n8.473106\n\n\n7\nmonth_5\n17.228385\n9.059851\n\n\n8\nmonth_6\n8.092298\n9.320895\n\n\n9\nmonth_7\n-9.132514\n10.481312\n\n\n10\nmonth_8\n15.455288\n10.181348\n\n\n11\nmonth_9\n34.746980\n9.060099\n\n\n12\nmonth_10\n13.061047\n8.383432\n\n\n13\nmonth_11\n-10.116635\n8.089260\n\n\n14\nmonth_12\n-6.009642\n6.397776\n\n\n15\nhr_1\n-18.318628\n6.855656\n\n\n16\nhr_2\n-26.524415\n6.844365\n\n\n17\nhr_3\n-40.579348\n7.037231\n\n\n18\nhr_4\n-35.211397\n6.946024\n\n\n19\nhr_5\n-21.257603\n6.830045\n\n\n20\nhr_6\n33.752431\n6.896172\n\n\n21\nhr_7\n175.926741\n6.850407\n\n\n22\nhr_8\n315.633093\n6.821242\n\n\n23\nhr_9\n164.763257\n6.907784\n\n\n24\nhr_10\n113.238368\n6.849508\n\n\n25\nhr_11\n134.600307\n6.906237\n\n\n26\nhr_12\n173.827217\n6.910010\n\n\n27\nhr_13\n166.014368\n7.187343\n\n\n28\nhr_14\n149.687063\n7.172399\n\n\n29\nhr_15\n166.504572\n7.102071\n\n\n30\nhr_16\n231.114910\n7.092921\n\n\n31\nhr_17\n373.796154\n7.034380\n\n\n32\nhr_18\n345.005402\n6.926682\n\n\n33\nhr_19\n236.632654\n6.900097\n\n\n34\nhr_20\n156.109006\n6.947926\n\n\n35\nhr_21\n109.951052\n6.801294\n\n\n36\nhr_22\n72.171886\n6.869633\n\n\n37\nhr_23\n35.564908\n6.826905\n\n\n38\nholiday_1\n-28.657206\n6.242407\n\n\n39\nweather_cond_Light_Snow_or_Light_Rain\n-67.006931\n4.147649\n\n\n40\nweather_cond_Mist_or_Cloudy\n-9.107550\n2.490628\n\n\n41\nwkday_monday\n12.929103\n3.888132\n\n\n42\nwkday_tuesday\n14.432173\n3.786723\n\n\n43\nwkday_wednesday\n16.434304\n3.769332\n\n\n44\nwkday_thursday\n16.408760\n3.787523\n\n\n45\nwkday_friday\n18.646276\n3.751100\n\n\n46\nwkday_saturday\n19.886551\n3.766310\n\n\n47\nseasons_summer\n42.795819\n6.230614\n\n\n48\nseasons_fall\n29.693335\n7.429177\n\n\n49\nseasons_winter\n68.685740\n6.289078\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nimport matplotlib.pyplot as plt\n\nthw = ['temp','hum','windspeed']\ndf_thw = df_cw8[df_cw8['features'].isin(thw)]\n\nplt.errorbar(df_thw['features'], df_thw['est'],\n             yerr = 1.96 * df_thw['std_error'], fmt = 'o', capsize = 5)\n\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model_d)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nmonths = []\nfor i in range(2, 13):\n  var = f'month_{i}'\n  months.append(var)\n\ndf_m = df_cw8[df_cw8['features'].isin(months)]\n\nplt.errorbar(df_m['features'], df_m['est'],\n             yerr = 1.96 * df_m['std_error'], fmt = 'o', capsize = 5)\n\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model_d)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nhrs = []\n\nfor i in range(1, 24):\n  hrs.append(f'hr_{i}')\n\ndf_h = df_cw8[df_cw8['features'].isin(hrs)]\n\nplt.errorbar(df_h['features'], df_h['est'],\n             yerr = 1.96 * df_h['std_error'], fmt = 'o', capsize = 5)\n\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model_d)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nday = ['monday', 'tuesday','wednesday', 'thursday', 'friday', 'saturday']\n\nwkd = []\n\nfor i in day:\n  wkd.append(f'wkday_{i}')\n\ndf_d = df_cw8[df_cw8['features'].isin(wkd)]\n\nplt.errorbar(df_d['features'], df_d['est'],\n             yerr = 1.96 * df_d['std_error'], fmt = 'o', capsize = 5)\n\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model_d)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nseasons = ['summer', 'fall', 'winter']\n\nsn = []\n\nfor i in seasons:\n  sn.append(f'seasons_{i}')\n\ndf_s = df_cw8[df_cw8['features'].isin(sn)]\n\nplt.errorbar(df_s['features'], df_s['est'],\n             yerr = 1.96 * df_s['std_error'], fmt = 'o', capsize = 5)\n\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model_d)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nexcess = thw + months + hrs + wkd + sn + ['year_2012', 'holiday_1']\n\ndf_w = df_cw8[~df_cw8['features'].isin(excess)]\n\nplt.errorbar(df_w['features'], df_w['est'],\n             yerr = 1.96 * df_w['std_error'], fmt = 'o', capsize = 5)\n\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model_d)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q8",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q8",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Q8",
    "text": "Q8\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm  # for lowess smoothing\n\n\ndf = dtest2.select([\"prediction\", \"cnt\"]).toPandas()\n# Pred and actual following fitting data to model, on test set specifically\ndf[\"residual\"] = df[\"cnt\"] - df[\"prediction\"]\n\nplt.scatter(df[\"prediction\"], df[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(df[\"residual\"], df[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n# Perfect prediction line\n#   A horizontal line at residual = 0\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n# Labeling\nplt.xlabel(\"Predicted sale.price (Model 2)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model 2\")\nplt.show()"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q9",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q9",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Q9",
    "text": "Q9\n\n# Create a histogram\nimport seaborn as sns\ndfpd =bikeshare.select([\"cnt\"]).toPandas()\nsns.histplot(dfpd[\"cnt\"], bins=10, kde=True)\n\n\n\n\n\n\n\n\n\ndfpd['log_cnt'] = np.log(dfpd['cnt'])\nsns.histplot(dfpd[\"log_cnt\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q10",
    "href": "danl_320_mats/danl_pysparklinreg/danl_320_lec_linreg.html#q10",
    "title": "DANL 320 | Linear Regression w/ PySpark",
    "section": "Q10",
    "text": "Q10\n\nbikeshare = pd.read_csv('https://bcdanl.github.io/data/bikeshare_cleaned.csv')\nbikeshare = spark.createDataFrame(bikeshare)\nbikeshare.show()\n\nbikeshare = bikeshare.withColumn(\"log_cnt\",\n                                 log(bikeshare['cnt']))\n\nbikeshare = bikeshare.withColumn(\"gp\", rand(seed=123)) # seed is set for replication\n\ndtrain, dtest = bikeshare.randomSplit([0.6, 0.4], seed = 123)\n\ndummy_cols_year, ref_cat_year = add_dummy_variables('year', 0)\ndummy_cols_month, ref_cat_month = add_dummy_variables('month', 0)\ndummy_cols_hr, ref_cat_hr = add_dummy_variables('hr', 0)\ndummy_cols_hol, ref_cat_hol = add_dummy_variables('holiday', 0)\ndummy_cols_wc, ref_cat_wc = add_dummy_variables('weather_cond', 0)\n\ncustom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\ndummy_cols_day, ref_cat_day = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\ncustom_order_season = ['spring', 'summer', 'fall', 'winter']\ndummy_cols_season, ref_cat_season = add_dummy_variables('seasons', reference_level=0, category_order = custom_order_season)\n\ncont_cols = ['temp', 'hum', 'windspeed']\n\nfeatures = cont_cols + dummy_cols_year + dummy_cols_month + dummy_cols_hr + dummy_cols_hol + dummy_cols_wc + dummy_cols_day + dummy_cols_season\n\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\n|cnt|year|month|date| hr|   wkday|holiday|seasons|        weather_cond|              temp|              hum|         windspeed|\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\n| 16|2011|    1|   1|  0|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.947345243330896|  -1.5538438052971|\n| 40|2011|    1|   1|  1|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n| 32|2011|    1|   1|  2|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n| 13|2011|    1|   1|  3|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n|  1|2011|    1|   1|  4|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n|  1|2011|    1|   1|  5|saturday|      0| spring|      Mist or Cloudy| -1.33460918694128|0.636351351217591|-0.821460017517193|\n|  2|2011|    1|   1|  6|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n|  3|2011|    1|   1|  7|saturday|      0| spring| Clear or Few Cloudy| -1.54234083286556| 1.20650682009198|  -1.5538438052971|\n|  8|2011|    1|   1|  8|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n| 14|2011|    1|   1|  9|saturday|      0| spring| Clear or Few Cloudy|-0.919145895092722|0.688183666569809|  -1.5538438052971|\n| 36|2011|    1|   1| 10|saturday|      0| spring| Clear or Few Cloudy|-0.607548426206302|0.688183666569809| 0.519881272378821|\n| 56|2011|    1|   1| 11|saturday|      0| spring| Clear or Few Cloudy|-0.711414249168442|0.947345243330896| 0.764281665845554|\n| 84|2011|    1|   1| 12|saturday|      0| spring| Clear or Few Cloudy|-0.399816780282022|0.740015981922026| 0.764281665845554|\n| 94|2011|    1|   1| 13|saturday|      0| spring|      Mist or Cloudy|-0.192085134357741|0.480854405160939| 0.886073166268775|\n|106|2011|    1|   1| 14|saturday|      0| spring|      Mist or Cloudy|-0.192085134357741|0.480854405160939| 0.764281665845554|\n|110|2011|    1|   1| 15|saturday|      0| spring|      Mist or Cloudy|-0.295950957319881|0.740015981922026| 0.886073166268775|\n| 93|2011|    1|   1| 16|saturday|      0| spring|      Mist or Cloudy|-0.399816780282022|0.999177558683113| 0.886073166268775|\n| 67|2011|    1|   1| 17|saturday|      0| spring|      Mist or Cloudy|-0.295950957319881|0.999177558683113| 0.764281665845554|\n| 35|2011|    1|   1| 18|saturday|      0| spring|Light Snow or Lig...|-0.399816780282022| 1.31017145079642| 0.519881272378821|\n| 37|2011|    1|   1| 19|saturday|      0| spring|Light Snow or Lig...|-0.399816780282022| 1.31017145079642| 0.519881272378821|\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\nonly showing top 20 rows\n\nReference category (dummy omitted): 2011\nReference category (dummy omitted): 1\nReference category (dummy omitted): 0\nReference category (dummy omitted): 0\nReference category (dummy omitted): Clear or Few Cloudy\nReference category (dummy omitted): sunday\nReference category (dummy omitted): spring\n\n\n\nassembler_lg = VectorAssembler(\n                    inputCols= features,\n                    outputCol=\"predictors\")\ndtrainlg = assembler_lg.transform(dtrain)\ndtestlg  = assembler_lg.transform(dtest)\n\nmodel_lg = LinearRegression(\n                featuresCol=\"predictors\",\n                labelCol=\"log_cnt\").fit(dtrainlg)\ndtestlg = model_lg.transform(dtestlg)\n\nprint(regression_table(model_lg, assembler_lg))\n\n+---------------------------------------------+--------+------+------------+---------+--------------+--------------+\n| Metric                                      |  Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+---------------------------------------------+--------+------+------------+---------+--------------+--------------+\n| Beta: temp                                  |  0.276 | ***  |      0.009 |   0.000 |        0.259 |        0.292 |\n| Beta: hum                                   | -0.048 | ***  |      0.007 |   0.000 |       -0.061 |       -0.035 |\n| Beta: windspeed                             | -0.030 | ***  |      0.012 |   0.000 |       -0.054 |       -0.005 |\n| Beta: year_2012                             |  0.482 | ***  |      0.031 |   0.000 |        0.421 |        0.543 |\n| Beta: month_2                               |  0.126 | ***  |      0.035 |   0.000 |        0.057 |        0.194 |\n| Beta: month_3                               |  0.156 | ***  |      0.052 |   0.000 |        0.054 |        0.258 |\n| Beta: month_4                               |  0.104 |  **  |      0.055 |   0.044 |       -0.004 |        0.213 |\n| Beta: month_5                               |  0.196 | ***  |      0.057 |   0.000 |        0.084 |        0.307 |\n| Beta: month_6                               |  0.101 |  *   |      0.064 |   0.078 |       -0.025 |        0.226 |\n| Beta: month_7                               | -0.081 |      |      0.062 |   0.207 |       -0.203 |        0.041 |\n| Beta: month_8                               |  0.034 |      |      0.055 |   0.590 |       -0.075 |        0.142 |\n| Beta: month_9                               |  0.099 |  *   |      0.051 |   0.074 |       -0.001 |        0.200 |\n| Beta: month_10                              | -0.015 |      |      0.050 |   0.769 |       -0.112 |        0.082 |\n| Beta: month_11                              | -0.076 |      |      0.039 |   0.125 |       -0.153 |        0.001 |\n| Beta: month_12                              | -0.075 |  *   |      0.042 |   0.057 |       -0.157 |        0.008 |\n| Beta: hr_1                                  | -0.602 | ***  |      0.042 |   0.000 |       -0.684 |       -0.520 |\n| Beta: hr_2                                  | -1.180 | ***  |      0.043 |   0.000 |       -1.265 |       -1.096 |\n| Beta: hr_3                                  | -1.734 | ***  |      0.043 |   0.000 |       -1.818 |       -1.651 |\n| Beta: hr_4                                  | -2.005 | ***  |      0.042 |   0.000 |       -2.087 |       -1.923 |\n| Beta: hr_5                                  | -0.948 | ***  |      0.042 |   0.000 |       -1.031 |       -0.865 |\n| Beta: hr_6                                  |  0.260 | ***  |      0.042 |   0.000 |        0.178 |        0.342 |\n| Beta: hr_7                                  |  1.319 | ***  |      0.042 |   0.000 |        1.237 |        1.400 |\n| Beta: hr_8                                  |  1.931 | ***  |      0.042 |   0.000 |        1.848 |        2.014 |\n| Beta: hr_9                                  |  1.601 | ***  |      0.042 |   0.000 |        1.519 |        1.683 |\n| Beta: hr_10                                 |  1.290 | ***  |      0.042 |   0.000 |        1.207 |        1.373 |\n| Beta: hr_11                                 |  1.383 | ***  |      0.042 |   0.000 |        1.300 |        1.466 |\n| Beta: hr_12                                 |  1.569 | ***  |      0.044 |   0.000 |        1.483 |        1.655 |\n| Beta: hr_13                                 |  1.529 | ***  |      0.044 |   0.000 |        1.443 |        1.615 |\n| Beta: hr_14                                 |  1.446 | ***  |      0.043 |   0.000 |        1.361 |        1.532 |\n| Beta: hr_15                                 |  1.534 | ***  |      0.043 |   0.000 |        1.449 |        1.619 |\n| Beta: hr_16                                 |  1.787 | ***  |      0.043 |   0.000 |        1.703 |        1.872 |\n| Beta: hr_17                                 |  2.161 | ***  |      0.042 |   0.000 |        2.078 |        2.244 |\n| Beta: hr_18                                 |  2.078 | ***  |      0.042 |   0.000 |        1.996 |        2.161 |\n| Beta: hr_19                                 |  1.787 | ***  |      0.043 |   0.000 |        1.704 |        1.871 |\n| Beta: hr_20                                 |  1.498 | ***  |      0.042 |   0.000 |        1.416 |        1.580 |\n| Beta: hr_21                                 |  1.284 | ***  |      0.042 |   0.000 |        1.201 |        1.366 |\n| Beta: hr_22                                 |  1.003 | ***  |      0.042 |   0.000 |        0.921 |        1.085 |\n| Beta: hr_23                                 |  0.633 | ***  |      0.038 |   0.000 |        0.558 |        0.708 |\n| Beta: holiday_1                             | -0.160 | ***  |      0.025 |   0.000 |       -0.210 |       -0.111 |\n| Beta: weather_cond_Light_Snow_or_Light_Rain | -0.599 | ***  |      0.015 |   0.000 |       -0.629 |       -0.569 |\n| Beta: weather_cond_Mist_or_Cloudy           | -0.040 | ***  |      0.024 |   0.010 |       -0.086 |        0.007 |\n| Beta: wkday_monday                          | -0.018 |      |      0.023 |   0.452 |       -0.063 |        0.028 |\n| Beta: wkday_tuesday                         | -0.036 |      |      0.023 |   0.124 |       -0.081 |        0.010 |\n| Beta: wkday_wednesday                       | -0.032 |      |      0.023 |   0.166 |       -0.077 |        0.013 |\n| Beta: wkday_thursday                        |  0.018 |      |      0.023 |   0.432 |       -0.027 |        0.063 |\n| Beta: wkday_friday                          |  0.119 | ***  |      0.023 |   0.000 |        0.074 |        0.164 |\n| Beta: wkday_saturday                        |  0.130 | ***  |      0.038 |   0.000 |        0.055 |        0.204 |\n| Beta: seasons_summer                        |  0.346 | ***  |      0.045 |   0.000 |        0.257 |        0.435 |\n| Beta: seasons_fall                          |  0.407 | ***  |      0.038 |   0.000 |        0.331 |        0.482 |\n| Beta: seasons_winter                        |  0.657 | ***  |      0.046 |   0.000 |        0.568 |        0.747 |\n| Intercept                                   |  3.100 | ***  |      0.014 |   0.000 |        3.072 |        3.129 |\n--------------------------------------------------------------------------------------------------------------------\n| Observations                                | 10,375 |      |            |         |              |              |\n| R²                                          |  0.825 |      |            |         |              |              |\n| RMSE                                        |  0.623 |      |            |         |              |              |\n+---------------------------------------------+--------+------+------------+---------+--------------+--------------+"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel A. Noone",
    "section": "",
    "text": "Senior at SUNY Geneseo majoring in Data Analytics. Academic interests lie in the fields of Artificial Intelligence and Machine Learning. Following the completion of my B.S. in Data Analytics I will be attending the Rochester Institute of Technology to pursue an M.S. in Artificial Intelligence."
  },
  {
    "objectID": "index.html#who-is-daniel-noone",
    "href": "index.html#who-is-daniel-noone",
    "title": "Daniel A. Noone",
    "section": "",
    "text": "Senior at SUNY Geneseo majoring in Data Analytics. Academic interests lie in the fields of Artificial Intelligence and Machine Learning. Following the completion of my B.S. in Data Analytics I will be attending the Rochester Institute of Technology to pursue an M.S. in Artificial Intelligence."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Daniel A. Noone",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. Data Analytics | Aug 2022 - May 2025 \nRochester Institute of Technology | Henrietta, NY  M.S. Artificial Intelligence | August 2025 - Undetermined"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Daniel A. Noone",
    "section": "Experience",
    "text": "Experience\nMuch experience in data collection and transformation, in both R and Python languages. Also well versed in the fundamental machine learning models and their optimization."
  }
]